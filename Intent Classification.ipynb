{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"Intent Classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f7T6KdX1eWID"},"source":["## For Colab"],"id":"f7T6KdX1eWID"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4_2pNYHjSpZ","executionInfo":{"status":"ok","timestamp":1634110101508,"user_tz":-420,"elapsed":5800,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"946c5a68-f05d-4071-dbd9-af10da83fa93"},"source":["!pip install yellowbrick"],"id":"R4_2pNYHjSpZ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: yellowbrick in /usr/local/lib/python3.7/dist-packages (0.9.1)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (0.10.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (3.2.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from yellowbrick) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10.0->yellowbrick) (1.15.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=1.5.1->yellowbrick) (2.8.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->yellowbrick) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDjthu7xsf0a","executionInfo":{"status":"ok","timestamp":1634096772388,"user_tz":-420,"elapsed":311,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"094cb02e-9c19-4a09-f6cc-e307d17c61d4"},"source":["%cd /content/drive/MyDrive/Intent-Detection/pretrained_embs/ELMo"],"id":"YDjthu7xsf0a","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Intent-Detection/pretrained_embs/ELMo\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGXK4DPUs-xB","executionInfo":{"status":"ok","timestamp":1634096772843,"user_tz":-420,"elapsed":12,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"e6941464-2695-43a6-d772-a216d6061db4"},"source":["!git clone https://github.com/hosjiu1702/ELMoForManyLangs.git"],"id":"UGXK4DPUs-xB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'ELMoForManyLangs' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMog-X8WtMMW","executionInfo":{"status":"ok","timestamp":1634096772843,"user_tz":-420,"elapsed":5,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"a10d5f50-b84c-4e76-8698-3e11e2e60593"},"source":["%cd ELMoForManyLangs/"],"id":"OMog-X8WtMMW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ix3ka30wtSl4","executionInfo":{"status":"ok","timestamp":1634096775415,"user_tz":-420,"elapsed":2574,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"1f367402-d02e-43e6-87b7-14c2b147c5c4"},"source":["!python setup.py install"],"id":"ix3ka30wtSl4","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["running install\n","running bdist_egg\n","running egg_info\n","writing elmoformanylangs.egg-info/PKG-INFO\n","writing dependency_links to elmoformanylangs.egg-info/dependency_links.txt\n","writing requirements to elmoformanylangs.egg-info/requires.txt\n","writing top-level names to elmoformanylangs.egg-info/top_level.txt\n","reading manifest template 'MANIFEST.in'\n","adding license file 'LICENSE'\n","writing manifest file 'elmoformanylangs.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","running build_py\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/__init__.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/__main__.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/biLM.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/dataloader.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/elmo.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/frontend.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","copying build/lib/elmoformanylangs/utils.py -> build/bdist.linux-x86_64/egg/elmoformanylangs\n","creating build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/__init__.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/classify_layer.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/elmo.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/embedding_layer.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/encoder_base.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/highway.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/lstm.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/lstm_cell_with_projection.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/token_embedder.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","copying build/lib/elmoformanylangs/modules/util.py -> build/bdist.linux-x86_64/egg/elmoformanylangs/modules\n","creating build/bdist.linux-x86_64/egg/elmoformanylangs/configs\n","copying build/lib/elmoformanylangs/configs/cnn_0_100_512_4096_sample.json -> build/bdist.linux-x86_64/egg/elmoformanylangs/configs\n","copying build/lib/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json -> build/bdist.linux-x86_64/egg/elmoformanylangs/configs\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/__main__.py to __main__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/biLM.py to biLM.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/dataloader.py to dataloader.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/elmo.py to elmo.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/frontend.py to frontend.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/utils.py to utils.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/__init__.py to __init__.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/classify_layer.py to classify_layer.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/elmo.py to elmo.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/embedding_layer.py to embedding_layer.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/encoder_base.py to encoder_base.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/highway.py to highway.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/lstm.py to lstm.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/lstm_cell_with_projection.py to lstm_cell_with_projection.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/token_embedder.py to token_embedder.cpython-37.pyc\n","byte-compiling build/bdist.linux-x86_64/egg/elmoformanylangs/modules/util.py to util.cpython-37.pyc\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","elmoformanylangs.__pycache__.elmo.cpython-37: module references __file__\n","creating 'dist/elmoformanylangs-0.0.4.post2-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing elmoformanylangs-0.0.4.post2-py3.7.egg\n","removing '/usr/local/lib/python3.7/dist-packages/elmoformanylangs-0.0.4.post2-py3.7.egg' (and everything under it)\n","creating /usr/local/lib/python3.7/dist-packages/elmoformanylangs-0.0.4.post2-py3.7.egg\n","Extracting elmoformanylangs-0.0.4.post2-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n","elmoformanylangs 0.0.4.post2 is already the active version in easy-install.pth\n","\n","Installed /usr/local/lib/python3.7/dist-packages/elmoformanylangs-0.0.4.post2-py3.7.egg\n","Processing dependencies for elmoformanylangs==0.0.4.post2\n","Searching for overrides==6.1.0\n","Best match: overrides 6.1.0\n","Processing overrides-6.1.0-py3.7.egg\n","overrides 6.1.0 is already the active version in easy-install.pth\n","\n","Using /usr/local/lib/python3.7/dist-packages/overrides-6.1.0-py3.7.egg\n","Searching for numpy==1.19.5\n","Best match: numpy 1.19.5\n","Adding numpy 1.19.5 to easy-install.pth file\n","Installing f2py script to /usr/local/bin\n","Installing f2py3 script to /usr/local/bin\n","Installing f2py3.7 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for h5py==3.1.0\n","Best match: h5py 3.1.0\n","Adding h5py 3.1.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for torch==1.9.0+cu111\n","Best match: torch 1.9.0+cu111\n","Adding torch 1.9.0+cu111 to easy-install.pth file\n","Installing convert-caffe2-to-onnx script to /usr/local/bin\n","Installing convert-onnx-to-caffe2 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for typing-utils==0.1.0\n","Best match: typing-utils 0.1.0\n","Processing typing_utils-0.1.0-py3.7.egg\n","typing-utils 0.1.0 is already the active version in easy-install.pth\n","\n","Using /usr/local/lib/python3.7/dist-packages/typing_utils-0.1.0-py3.7.egg\n","Searching for cached-property==1.5.2\n","Best match: cached-property 1.5.2\n","Adding cached-property 1.5.2 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Searching for typing-extensions==3.7.4.3\n","Best match: typing-extensions 3.7.4.3\n","Adding typing-extensions 3.7.4.3 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.7/dist-packages\n","Finished processing dependencies for elmoformanylangs==0.0.4.post2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8Ut8njUuNi7","executionInfo":{"status":"ok","timestamp":1634096775416,"user_tz":-420,"elapsed":15,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"875e06d9-770b-4a53-fbcc-7f6245b12253"},"source":["!pwd"],"id":"c8Ut8njUuNi7","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X7GTFwf3v7Bx","executionInfo":{"status":"ok","timestamp":1634096775416,"user_tz":-420,"elapsed":9,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"ddef3c0e-4967-478e-96f9-74f9e71bc8e1"},"source":["%cd /content/drive/MyDrive/Intent-Detection"],"id":"X7GTFwf3v7Bx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Intent-Detection\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-V2_2kxwIP-","executionInfo":{"status":"ok","timestamp":1634096779294,"user_tz":-420,"elapsed":3883,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"c4dfb068-f340-49d1-b367-6126c50482fb"},"source":["!pwd\n","!ls -l\n","!pip install -r requirements.txt"],"id":"L-V2_2kxwIP-","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Intent-Detection\n","total 168\n","drwx------ 2 root root   4096 Oct 13 03:25  data\n","-rw------- 1 root root 162131 Oct 13 03:45 'Intent Classification.ipynb'\n","drwx------ 3 root root   4096 Oct 13 03:25  pretrained_embs\n","-rw------- 1 root root     18 Oct 13 03:25  README.md\n","-rw------- 1 root root     34 Oct 13 03:25  requirements.txt\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.0.0)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.5)\n","Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.19.5)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (7.6.5)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (4.10.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.6.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.3.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->-r requirements.txt (line 1)) (5.2.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 2)) (0.22.2.post1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi->-r requirements.txt (line 4)) (0.3.6)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (5.1.1)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (5.5.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (5.3.5)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 1)) (5.1.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (57.4.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (0.8.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (4.4.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 1)) (0.2.5)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (1.0.2)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 1)) (5.1.3)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 1)) (2.6.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 1)) (4.8.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 1)) (1.8.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 1)) (2.11.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->-r requirements.txt (line 1)) (0.12.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->-r requirements.txt (line 1)) (22.3.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->-r requirements.txt (line 1)) (0.7.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->-r requirements.txt (line 1)) (2.0.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.7.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (4.1.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.5.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 1)) (0.3)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 1)) (0.5.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 1)) (21.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->jupyter->-r requirements.txt (line 1)) (2.4.7)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 1)) (1.11.2)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 2)) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 2)) (1.0.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi->-r requirements.txt (line 4)) (0.8.9)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi->-r requirements.txt (line 4)) (4.62.3)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi->-r requirements.txt (line 4)) (0.9.7)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttZNOCjU1knK","executionInfo":{"status":"ok","timestamp":1634098257555,"user_tz":-420,"elapsed":6674,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"acb5912b-d884-47f1-8445-918f097aa2d7"},"source":["!sudo apt install git-lfs"],"id":"ttZNOCjU1knK","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  git-lfs\n","0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 2,129 kB of archives.\n","After this operation, 7,662 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n","Fetched 2,129 kB in 1s (1,871 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package git-lfs.\n","(Reading database ... 155047 files and directories currently installed.)\n","Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n","Unpacking git-lfs (2.3.4-1) ...\n","Setting up git-lfs (2.3.4-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atVMDGmH2KaO","executionInfo":{"status":"ok","timestamp":1634098298019,"user_tz":-420,"elapsed":16037,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"746929a0-4b26-4cc6-e224-d3bc59e8918c"},"source":["!git-lfs fetch"],"id":"atVMDGmH2KaO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fetching master\n","Git LFS: (2 of 2 files) 393.35 MB / 393.35 MB\n"]}]},{"cell_type":"code","metadata":{"id":"9rSPbixB24Yi"},"source":["!git-lfs pull"],"id":"9rSPbixB24Yi","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBLm7WXkl_J8"},"source":["## Imports"],"id":"yBLm7WXkl_J8"},{"cell_type":"code","metadata":{"id":"78602309"},"source":["from collections import namedtuple\n","from pprint import pprint\n","from typing import Iterable, List\n","\n","import pandas as pd\n","import numpy as np\n","from pyvi import ViTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.svm import LinearSVC, SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.model_selection import (\n","    GridSearchCV,\n","    learning_curve,\n","    train_test_split\n",")\n","from sklearn.pipeline import Pipeline\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n","from sklearn.base import TransformerMixin, BaseEstimator\n","from sklearn.calibration import CalibratedClassifierCV\n","from elmoformanylangs import Embedder\n","import matplotlib.pyplot as plt\n","from yellowbrick.classifier import PrecisionRecallCurve, ClassificationReport"],"id":"78602309","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78af2417"},"source":["## Text Extraction"],"id":"78af2417"},{"cell_type":"code","metadata":{"id":"5dc14d99"},"source":["# Load raw data\n","file_path = \"./data/raw/data.xlsx\"\n","df = pd.read_excel(file_path, engine=\"openpyxl\", sheet_name=1)"],"id":"5dc14d99","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":625},"id":"774d0b26","executionInfo":{"status":"ok","timestamp":1634096789886,"user_tz":-420,"elapsed":19,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"9877a7df-94ed-4a4e-d853-1edc2c715497"},"source":["df.head()"],"id":"774d0b26","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fileName</th>\n","      <th>start</th>\n","      <th>end</th>\n","      <th>speaker</th>\n","      <th>sentiment</th>\n","      <th>text</th>\n","      <th>Intent</th>\n","      <th>Unnamed: 7</th>\n","      <th>Unnamed: 8</th>\n","      <th>Unnamed: 9</th>\n","      <th>Unnamed: 10</th>\n","      <th>Unnamed: 11</th>\n","      <th>Unnamed: 12</th>\n","      <th>Unnamed: 13</th>\n","      <th>Unnamed: 14</th>\n","      <th>Unnamed: 15</th>\n","      <th>Unnamed: 16</th>\n","      <th>Unnamed: 17</th>\n","      <th>Unnamed: 18</th>\n","      <th>Unnamed: 19</th>\n","      <th>Unnamed: 20</th>\n","      <th>Unnamed: 21</th>\n","      <th>Unnamed: 22</th>\n","      <th>Unnamed: 23</th>\n","      <th>Unnamed: 24</th>\n","      <th>Unnamed: 25</th>\n","      <th>Unnamed: 26</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>VUQC83EOO969H66VRLO5RMEPC8089Q00_2021-07-16_11...</td>\n","      <td>0.81</td>\n","      <td>1.27</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>a lô</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>1.86</td>\n","      <td>2.76</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>anh đơn hả anh</td>\n","      <td>Agent_VerifyCustomerName</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>4.62</td>\n","      <td>7.86</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>ờ anh em là hương nè em gọi đến cho anh từ bên...</td>\n","      <td>Agent_Self Introduction</td>\n","      <td>Agent_CompanyIntroduction</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>8.04</td>\n","      <td>34.32</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>a lô cho em hỏi xíu ha là vợ anh chị lép á còn...</td>\n","      <td>Agent_VerifyCustomerName</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>34.68</td>\n","      <td>35.07</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>anh anh</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            fileName  ... Unnamed: 26\n","0  VUQC83EOO969H66VRLO5RMEPC8089Q00_2021-07-16_11...  ...         NaN\n","1                                                NaN  ...         NaN\n","2                                                NaN  ...         NaN\n","3                                                NaN  ...         NaN\n","4                                                NaN  ...         NaN\n","\n","[5 rows x 27 columns]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"497114a0"},"source":["# Drops some unnecessary columns\n","df = df.drop(columns=[\"fileName\", \"start\", \"end\", \"speaker\", \"sentiment\"])"],"id":"497114a0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":608},"id":"4357dd65","executionInfo":{"status":"ok","timestamp":1634096789887,"user_tz":-420,"elapsed":11,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"fe5dc12b-f067-4ea4-d426-99ca45799956"},"source":["df.head()"],"id":"4357dd65","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>Intent</th>\n","      <th>Unnamed: 7</th>\n","      <th>Unnamed: 8</th>\n","      <th>Unnamed: 9</th>\n","      <th>Unnamed: 10</th>\n","      <th>Unnamed: 11</th>\n","      <th>Unnamed: 12</th>\n","      <th>Unnamed: 13</th>\n","      <th>Unnamed: 14</th>\n","      <th>Unnamed: 15</th>\n","      <th>Unnamed: 16</th>\n","      <th>Unnamed: 17</th>\n","      <th>Unnamed: 18</th>\n","      <th>Unnamed: 19</th>\n","      <th>Unnamed: 20</th>\n","      <th>Unnamed: 21</th>\n","      <th>Unnamed: 22</th>\n","      <th>Unnamed: 23</th>\n","      <th>Unnamed: 24</th>\n","      <th>Unnamed: 25</th>\n","      <th>Unnamed: 26</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>a lô</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>anh đơn hả anh</td>\n","      <td>Agent_VerifyCustomerName</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ờ anh em là hương nè em gọi đến cho anh từ bên...</td>\n","      <td>Agent_Self Introduction</td>\n","      <td>Agent_CompanyIntroduction</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>a lô cho em hỏi xíu ha là vợ anh chị lép á còn...</td>\n","      <td>Agent_VerifyCustomerName</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>anh anh</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  ... Unnamed: 26\n","0                                               a lô  ...         NaN\n","1                                     anh đơn hả anh  ...         NaN\n","2  ờ anh em là hương nè em gọi đến cho anh từ bên...  ...         NaN\n","3  a lô cho em hỏi xíu ha là vợ anh chị lép á còn...  ...         NaN\n","4                                            anh anh  ...         NaN\n","\n","[5 rows x 22 columns]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"faa5d0cf"},"source":["Dataset = namedtuple(\"Dataset\", [\"sentences\", \"labels\"])\n","dataset = Dataset([], [])\n","\n","for _, row in df.iterrows():\n","    # Avoid NaN inside text\n","    if isinstance(row[\"text\"], str):\n","        dataset.sentences.append(row[\"text\"])\n","        labels = []\n","        for i, val in row.iteritems():\n","            if i == \"text\" or isinstance(val, float):\n","                continue\n","            labels.append(val)\n","        dataset.labels.append(labels)"],"id":"faa5d0cf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"193aedc3","executionInfo":{"status":"ok","timestamp":1634096791290,"user_tz":-420,"elapsed":25,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"c0939545-1be0-4f80-a61e-e923b2c7b82d"},"source":["dataset.sentences[:2]"],"id":"193aedc3","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['a lô', 'anh đơn hả anh']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95b6ac3f","executionInfo":{"status":"ok","timestamp":1634096791290,"user_tz":-420,"elapsed":20,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"d5dc9951-44ca-4ac1-b073-6ac932f53146"},"source":["dataset.labels[:2]"],"id":"95b6ac3f","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[], ['Agent_VerifyCustomerName']]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"26e65df7","executionInfo":{"status":"ok","timestamp":1634096791291,"user_tz":-420,"elapsed":18,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"8a3f6cf5-5c23-43a4-a5d4-a3ff1c7c8d23"},"source":["len(dataset.sentences), len(dataset.labels)"],"id":"26e65df7","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8173, 8173)"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"7762a08e"},"source":["## Text Cleaning"],"id":"7762a08e"},{"cell_type":"markdown","metadata":{"id":"07c9ddbc"},"source":["### Remove duplication"],"id":"07c9ddbc"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ecd383d8","executionInfo":{"status":"ok","timestamp":1634096791291,"user_tz":-420,"elapsed":15,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"762ce045-2441-441f-c955-572733c73189"},"source":["\"\"\" Data Cleaning\n","\"\"\"\n","\n","# Convert to pandas dataframe from namedtuple\n","d = dict({\n","    \"sentences\": [],\n","    \"labels\": []\n","})\n","sentences = dataset.sentences\n","labels = dataset.labels\n","for sent, lb in zip(sentences, labels):\n","    # update dict\n","    d[\"sentences\"].append(sent)\n","    d[\"labels\"].append(lb)\n","for idx, item in enumerate(d[\"labels\"]):\n","    if len(item) == 0:\n","        d[\"labels\"][idx] = \"other\"\n","    else:\n","        d[\"labels\"][idx] = \" | \".join(item)\n","\n","dataset_df = pd.DataFrame.from_dict(d)\n","\n","# Take advantage pandas utility for text cleaning\n","## remove duplicate\n","print(f\"Before drop: {len(dataset_df.index)}\", end=\"\\n\\n\")\n","dataset_df.drop_duplicates(inplace=True, ignore_index=True)\n","print(f\"After drop: {len(dataset_df.index)}\")"],"id":"ecd383d8","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before drop: 8173\n","\n","After drop: 6167\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"81900e95","executionInfo":{"status":"ok","timestamp":1634096791292,"user_tz":-420,"elapsed":12,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"0e53ba22-3d7f-4102-d939-933234fef2ce"},"source":["dataset_df.head(20)"],"id":"81900e95","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentences</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>a lô</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>anh đơn hả anh</td>\n","      <td>Agent_VerifyCustomerName</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ờ anh em là hương nè em gọi đến cho anh từ bên...</td>\n","      <td>Agent_Self Introduction | Agent_CompanyIntrodu...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>a lô cho em hỏi xíu ha là vợ anh chị lép á còn...</td>\n","      <td>Agent_VerifyCustomerName</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>anh anh</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ừm</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>anh đưa điện thoại giùm em</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>à chị lép hả chị</td>\n","      <td>Agent_Self Introduction</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>à hiện tại hiện thì trong cái đợt dịch đợt dịc...</td>\n","      <td>Agent_CallPurpose</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>cái điện số điện thoại cũ của chị thì em gọi n...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>là hồ sơ của chị lép á lần lần trước là bên em...</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>lần đó là mình góp tốt á chị</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>nên là lần này à mình được bên em hỗ trợ á một...</td>\n","      <td>Agent_CallPurpose</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>bốn mươi tám triệu với lãi suất thấp</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>dạ</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>một triệu á chị lép còn có mười sáu ngàn thôi á</td>\n","      <td>Agent_InterestRate</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>em ví dụ á mà mua mà</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>dạ bốn mươi lăm mà định</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>chị đợi em tính xí nha</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>rồi chị đợi</td>\n","      <td>other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentences                                             labels\n","0                                                a lô                                              other\n","1                                      anh đơn hả anh                           Agent_VerifyCustomerName\n","2   ờ anh em là hương nè em gọi đến cho anh từ bên...  Agent_Self Introduction | Agent_CompanyIntrodu...\n","3   a lô cho em hỏi xíu ha là vợ anh chị lép á còn...                           Agent_VerifyCustomerName\n","4                                             anh anh                                              other\n","5                                                  ừm                                              other\n","6                          anh đưa điện thoại giùm em                                              other\n","7                                    à chị lép hả chị                            Agent_Self Introduction\n","8   à hiện tại hiện thì trong cái đợt dịch đợt dịc...                                  Agent_CallPurpose\n","9   cái điện số điện thoại cũ của chị thì em gọi n...                                              other\n","10  là hồ sơ của chị lép á lần lần trước là bên em...                                              other\n","11                       lần đó là mình góp tốt á chị                                              other\n","12  nên là lần này à mình được bên em hỗ trợ á một...                                  Agent_CallPurpose\n","13               bốn mươi tám triệu với lãi suất thấp                                              other\n","14                                                 dạ                                              other\n","15    một triệu á chị lép còn có mười sáu ngàn thôi á                                 Agent_InterestRate\n","16                               em ví dụ á mà mua mà                                              other\n","17                            dạ bốn mươi lăm mà định                                              other\n","18                             chị đợi em tính xí nha                                              other\n","19                                        rồi chị đợi                                              other"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"8b2613a9"},"source":["## Text Pre-processing"],"id":"8b2613a9"},{"cell_type":"markdown","metadata":{"id":"7f98a806"},"source":["### Tokenization"],"id":"7f98a806"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"09483e0f","executionInfo":{"status":"ok","timestamp":1634096794564,"user_tz":-420,"elapsed":3282,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"f2c99cee-aee7-4824-df05-984041eef21c"},"source":["for idx, row in dataset_df.iterrows():\n","    row[\"sentences\"] = ViTokenizer.tokenize(row[\"sentences\"])\n","dataset_df.head(10)"],"id":"09483e0f","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentences</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>a_lô</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>anh đơn hả anh</td>\n","      <td>Agent_VerifyCustomerName</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ờ anh_em là hương nè em gọi đến cho anh từ bên...</td>\n","      <td>Agent_Self Introduction | Agent_CompanyIntrodu...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>a_lô cho em hỏi xíu ha là vợ anh_chị lép á còn...</td>\n","      <td>Agent_VerifyCustomerName</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>anh anh</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ừm</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>anh đưa điện_thoại giùm em</td>\n","      <td>other</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>à chị lép hả chị</td>\n","      <td>Agent_Self Introduction</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>à hiện_tại hiện thì trong cái đợt dịch đợt dịc...</td>\n","      <td>Agent_CallPurpose</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>cái điện số điện_thoại cũ của chị thì em gọi n...</td>\n","      <td>other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           sentences                                             labels\n","0                                               a_lô                                              other\n","1                                     anh đơn hả anh                           Agent_VerifyCustomerName\n","2  ờ anh_em là hương nè em gọi đến cho anh từ bên...  Agent_Self Introduction | Agent_CompanyIntrodu...\n","3  a_lô cho em hỏi xíu ha là vợ anh_chị lép á còn...                           Agent_VerifyCustomerName\n","4                                            anh anh                                              other\n","5                                                 ừm                                              other\n","6                         anh đưa điện_thoại giùm em                                              other\n","7                                   à chị lép hả chị                            Agent_Self Introduction\n","8  à hiện_tại hiện thì trong cái đợt dịch đợt dịc...                                  Agent_CallPurpose\n","9  cái điện số điện_thoại cũ của chị thì em gọi n...                                              other"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"3f666333"},"source":["## Modeling"],"id":"3f666333"},{"cell_type":"markdown","metadata":{"id":"nXNBXCdPcDRp"},"source":["### Data Preparation"],"id":"nXNBXCdPcDRp"},{"cell_type":"code","metadata":{"id":"d42384a2"},"source":["\"\"\" We transform multi-label classification into\n","Multiclass classificaiton using OvR (One vs Rest) strategy\n","\"\"\"\n","X = X = dataset_df[\"sentences\"].tolist()\n","y = []\n","items = dataset_df[\"labels\"].tolist()\n","for item in items:\n","    labels = item.split(\" | \")\n","    y.append(labels)\n","\n","# Labels Binarization\n","mlb = MultiLabelBinarizer()\n","y = mlb.fit_transform(y)\n","\n","# Using train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size=0.2,\n","                                                    random_state=42)"],"id":"d42384a2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-O0cWIEZ86n","executionInfo":{"status":"ok","timestamp":1634114293438,"user_tz":-420,"elapsed":7,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"a5a9cf9e-7e53-45e5-968d-aef60a121f08"},"source":["print(\"Total samples: %d\" % len(X))\n","print(\"Train size: %d\" % len(X_train))\n","print(\"Test size: %d\" % len(X_test))"],"id":"s-O0cWIEZ86n","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total samples: 6167\n","Train size: 4933\n","Test size: 1234\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"705773b2","executionInfo":{"status":"ok","timestamp":1634108702505,"user_tz":-420,"elapsed":275,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"5afa8563-5e30-4815-fc61-9db6e310c256"},"source":["pprint(X[10:15])\n","pprint(X_train[10:15])\n","pprint(X_test[10:15])"],"id":"705773b2","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['là hồ_sơ của chị lép á lần lần trước là bên em cũng có hỗ_trợ cho mình mua '\n"," 'trả_góp cái điện_thoại_vi vô ở bạc liêu nè',\n"," 'lần đó là mình góp tốt á chị',\n"," 'nên là lần này à mình được bên em hỗ_trợ á một cái khoản vay tiêu_dùng từ '\n"," 'mười cho tới à',\n"," 'bốn_mươi tám triệu với lãi_suất thấp',\n"," 'dạ']\n","['chụp hình chứng_minh nhân_dân mặt trước mà sau ảnh khuôn_mặt của anh xong '\n"," 'đến cái mức kí nó sẽ mở cái hợp_đồng ra cho anh coi anh hiểu không anh đọc '\n"," 'anh coi',\n"," 'a không chị giữ máy giúp em để em xác_minh thông_tin của mình hen là tên họ '\n"," 'đầy_đủ của mình là nguyễn thị non hả chị',\n"," 'à bình bình tân thì cũng dịch',\n"," 'ừm à vậy á hả ủa con_con_cái số mới của của',\n"," 'dạ không biết là mình']\n","['cái lãi_suất của chị rất thấp một triệu có mười bốn ngàn à nếu_như mà lấy '\n"," 'mười triệu thì mỗi tháng góp cho bên em có một triệu không trăm hai mươi tám '\n"," 'ngàn à',\n"," 'dạ sao vậy anh_em thấy cái lãi_suất',\n"," 'nếu mà liên_hệ cái người tên sơn không được thì mới gọi qua em giống như chị '\n"," 'nè chị gọi cho sơn không được là chị',\n"," 'chị làm hồ_sơ cho em và em chỉ_xác_minh lại cái cái chứng_minh nhân_dân của '\n"," 'mình',\n"," 'nhưng mà mình mình vô đó mấy ngày rồi']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"96f00837","executionInfo":{"status":"ok","timestamp":1634096794567,"user_tz":-420,"elapsed":40,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"ebdccbca-5966-4274-9306-ca5479ea62d8"},"source":["# (samples, labels)\n","y.shape"],"id":"96f00837","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6167, 38)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9af3eefa","executionInfo":{"status":"ok","timestamp":1634096794567,"user_tz":-420,"elapsed":13,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"ca733048-1620-4864-b567-fed101c7b502"},"source":["### Unique classes\n","mlb.classes_, len(mlb.classes_)"],"id":"9af3eefa","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['Agent_CallPurpose', 'Agent_ChildEducation',\n","        'Agent_CompanyIntroduction', 'Agent_EMI',\n","        'Agent_ExplainDocsRequired', 'Agent_ExplainMoneyTranferProcess',\n","        'Agent_FamilyShopping', 'Agent_GoodClose', 'Agent_Greetings',\n","        'Agent_HouseFixing', 'Agent_InformCallBack', 'Agent_InsuranceFee',\n","        'Agent_InterestRate', 'Agent_ListeningSkill', 'Agent_LoanAmount',\n","        'Agent_LoanDuration', 'Agent_MentioningDisclaimer',\n","        'Agent_OH_AmountLess', 'Agent_OH_BadExperience',\n","        'Agent_OH_CustAskForCallingBack', 'Agent_OH_DiscussWithFamily',\n","        'Agent_OH_HighInterest', 'Agent_OH_HighLoanDuration',\n","        'Agent_OH_NotRequireInsurance', 'Agent_OH_NotRequireLoan',\n","        'Agent_RunBusiness', 'Agent_Self Introduction', 'Agent_Summarize',\n","        'Agent_ThanksClosing', 'Agent_VerifyCustomerName',\n","        'Client_AmountLess', 'Client_AskForCallingBack',\n","        'Client_BadExperience', 'Client_DiscussWithFamily',\n","        'Client_HighInterest', 'Client_HighLoanDuration',\n","        'Client_NotRequireLoan', 'other'], dtype=object), 38)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"82e346ef"},"source":["## Feature Extraction"],"id":"82e346ef"},{"cell_type":"markdown","metadata":{"id":"8a0a4b63"},"source":["### Count-based vectorizer"],"id":"8a0a4b63"},{"cell_type":"markdown","metadata":{"id":"85f608e5"},"source":["#### Tf-idf"],"id":"85f608e5"},{"cell_type":"code","metadata":{"id":"bf762612"},"source":["# Tf-idf\n","tfidf_vect = TfidfVectorizer()\n","# tfidf_vect = tfidf_vect.fit(X)"],"id":"bf762612","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"15d3b7df"},"source":["### Custom Embeddings"],"id":"15d3b7df"},{"cell_type":"markdown","metadata":{"id":"defd071c"},"source":["#### Example"],"id":"defd071c"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"53cef44b","executionInfo":{"status":"ok","timestamp":1634096794568,"user_tz":-420,"elapsed":10,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"8cbb84ba-8c0a-41fc-ae3d-817d07fea83b"},"source":["\"\"\"\n","from sklearn.datasets import load_iris\n","from sklearn.pipeline import make_pipeline\n","\n","\n","class MyTransformer(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X):\n","        return X\n","\n","\n","X, y = load_iris(return_X_y=True)\n","pipe = make_pipeline(MyTransformer(),\n","                     LogisticRegression(solver=\"saga\"))\n","pipe.fit(X, y)\n","\"\"\""],"id":"53cef44b","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.pipeline import make_pipeline\\n\\n\\nclass MyTransformer(BaseEstimator, TransformerMixin):\\n    def fit(self, X, y=None):\\n        return self\\n\\n    def transform(self, X):\\n        return X\\n\\n\\nX, y = load_iris(return_X_y=True)\\npipe = make_pipeline(MyTransformer(),\\n                     LogisticRegression(solver=\"saga\"))\\npipe.fit(X, y)\\n'"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"b6ae58fc"},"source":["#### ELMo Embeddings"],"id":"b6ae58fc"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eeb2671","executionInfo":{"status":"ok","timestamp":1634098587647,"user_tz":-420,"elapsed":8480,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"148bdc2b-823a-4350-d7e3-ea74b445c363"},"source":["class ELMoEmbedder(BaseEstimator, TransformerMixin):\n","    def __init__(self, model_dir):\n","        self.model_dir = model_dir\n","        self.embedder = Embedder(self.model_dir)\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X):\n","        \"\"\"\n","        Arguments:\n","            X: Iterable[Iterable] input texts\n","        \"\"\"\n","        if not isinstance(X, List):\n","            raise ValueError(\"Your input data should be a list\")\n","        \n","        if not isinstance(X[0], List):\n","            X = [X]\n","\n","        embeddings = self.embedder.sents2elmo(X)\n","        \n","        return np.concatenate(embeddings)\n","\n","\n","texts = [\n","    [\"em đang nơi đâu\"],\n","    [\"chúng ta của hiện tại\"],\n","    [\"muộn rồi mà sao còn\"]\n","]\n","\n","model_dir = \"pretrained_embs/ELMo/vi/\"\n","elmo = ELMoEmbedder(model_dir)\n","# outputs = elmo.sents2elmo(texts)\n","outputs = elmo.transform(texts)\n","pprint(outputs)"],"id":"9eeb2671","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-10-13 04:16:19,066 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 04:16:19,069 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 04:16:19,094 INFO: char embedding size: 3920\n","2021-10-13 04:16:19,568 INFO: word embedding size: 88497\n","2021-10-13 04:16:26,414 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 04:16:27,146 INFO: 1 batches, avg len: 3.0\n"]},{"output_type":"stream","name":"stdout","text":["array([[-0.06334513, -0.01504875, -0.09831727, ...,  0.12664728,\n","        -0.11577673, -0.32729256],\n","       [-0.03447661,  0.25508228, -0.28790745, ...,  0.2594107 ,\n","         0.3580422 , -0.04250387],\n","       [ 0.19482063,  0.33998236, -0.28850168, ...,  0.16124493,\n","        -0.05382353, -0.41921782]], dtype=float32)\n"]}]},{"cell_type":"markdown","metadata":{"id":"4f78c8d9"},"source":["### Model Training"],"id":"4f78c8d9"},{"cell_type":"code","metadata":{"scrolled":false,"id":"2e9df376"},"source":["\"\"\" We use Grid Search for doing hyperparameters tuning \"\"\"\n","\n","# Cast multi-label problem to multi-class classification one.\n","\n","# ## setting 1\n","# pipe1 = Pipeline([\n","#     (\"vect\", tfidf_vect),\n","#     (\"clf\", OneVsRestClassifier(LogisticRegression(), n_jobs=-1))\n","# ])\n","# grid1 = [\n","#     {\n","#         \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3), (1, 3)],\n","#         \"vect__max_df\": [0.5, 0.75, 1.0],\n","#         \"clf__estimator__max_iter\": [50, 100, 1000, 5000],\n","#         \"clf__estimator__C\": [0.001, 0.01, 0.1, 1., 10, 100],\n","#         \"clf__estimator__class_weight\": [\"balanced\", None]\n","#     }\n","# ]\n","\n","## setting 2\n","pipe2 = Pipeline([\n","    (\"vect\", tfidf_vect),\n","    # (\"scaler\", StandardScaler(with_mean=False)),\n","    (\"clf\", OneVsRestClassifier(LinearSVC(), n_jobs=-1))\n","], memory=\"cache/tfidf\")\n","grid2 = [\n","    {\n","        # \"vect__ngram_range\": [(1, 2)],\n","        # \"vect__max_df\": [0.5],\n","        # \"clf__estimator__class_weight\": [\"balanced\"]\n","        \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3)],\n","        \"vect__max_df\": [0.2, 0.5, 0.75, 1.0],\n","        \"vect__max_features\": [25000, 50000, 75000],\n","        \"clf__estimator__C\": [0.01, 1, 10, 100],\n","        \"clf__estimator__class_weight\": [\"balanced\", None]\n","    }\n","]\n","\n","# ## setting 3\n","# pipe3 = Pipeline([\n","#     (\"vect\", tfidf_vect),\n","#     (\"clf\", OneVsRestClassifier(MultinomialNB(), n_jobs=-1))\n","# ])\n","# grid3 = [\n","#     {\n","#         \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3), (1, 3)],\n","#         \"vect__max_df\": [0.5, 0.75, 1.0],\n","#         \"clf__estimator__alpha\": [0.01, 0.1, 1, 10, 100]\n","#     }\n","# ]\n","\n","## setting 4\n","# pipe4 = Pipeline([\n","#     (\"vect\", elmo),\n","#     (\"clf\", OneVsRestClassifier(LinearSVC(), n_jobs=-1))\n","# ])\n","# grid4 = [\n","#     {\n","#         \"clf__estimator__alpha\": [0.01, 0.1, 1, 10, 100]\n","#     }\n","# ]\n","\n","## Apply grid search\n","pipes = [pipe2]\n","grids = [grid2]\n","best_score = 0.\n","best_params = None\n","best_estimator = None\n","for pipe, grid in zip(pipes, grids):\n","    gridcv = GridSearchCV(estimator=pipe,\n","                          param_grid=grid,\n","                          scoring=\"f1_micro\",\n","                          n_jobs=-1,\n","                          return_train_score=True)\n","    gridcv.fit(X_train, y_train)\n","    if gridcv.best_score_ > best_score:\n","        best_score = gridcv.best_score_\n","        best_params = gridcv.best_params_\n","        best_estimator = gridcv.best_estimator_\n","\n","## Print some useful infos\n","print(f\"best score: {gridcv.best_score_:.3f}\", end=\"\\n\\n\")\n","pprint(f\"best params: {gridcv.best_params_}\")\n","clf = gridcv.best_estimator_\n","\n","\n","# Grid params\n","# grid_params = [\n","#     # setting 1\n","#     {\n","#         \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3), (1, 3)],\n","#         \"vect__max_df\": [0.5, 0.75, 1.0],\n","#         \"vect__min_df\": [0.5, 0.75, 1.0],\n","#         \"clf\": [OneVsRestClassifier(LogisticRegression(n_jobs=-1))], # Logistic Regression\n","#         \"clf__max_iter\": [50, 100, 1000, 5000],\n","#         \"clf__C\": [0.001, 0.01, 0.1, 1., 10, 100],\n","#         \"clf__class_weight\": [\"balanced\", None]\n","#     },\n","\n","#     # setting 2\n","#     {\n","#         \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3), (1, 3)],\n","#         \"vect__max_df\": [0.5, 0.75, 1.0],\n","#         \"vect__min_df\": [0.5, 0.75, 1.0],\n","#         \"clf\": [OneVsRestClassifier(LinearSVC())], # SVM with linear kernel\n","#         \"clf__C\": [0.001, 0.01, 1, 10, 100],\n","#         \"clf__class_weight\": [\"balanced\", None]\n","#     },\n","\n","#     # setting 3\n","#     {\n","#         \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2), (2, 3), (1, 3)],\n","#         \"vect__max_df\": [0.5, 0.75, 1.0],\n","#         \"vect__min_df\": [0.5, 0.75, 1.0],\n","#         \"clf\": [OneVsRestClassifier(MultinomialNB())], # Multinomial Naive Bayes\n","#         \"clf__alpha\": [0.01, 0.1, 1, 10, 100]\n","#     }\n","# ]\n","\n","# Apply grid search\n","# grid = GridSearchCV(estimator=pipe,\n","#                     param_grid=grid_params,\n","#                     scoring=\"f1\")\n","# grid.fit(X, y)\n","# best_estimator = grid.best_estimator_\n","# print(f\"best score: {grid.best_score_:.3f}\", end=\"\\n\\n\")\n","# pprint(f\"best params: {grid.best_params_}\")\n","\n","# clf = Pipeline([\n","#     (\"vect\", elmo),\n","#     # (\"scaler\", StandardScaler()),\n","#     (\"clf\", OneVsRestClassifier(LogisticRegression(random_state=1337, class_weight=\"balanced\"), n_jobs=-1)),\n","# ], memory=\"cache/elmo\")\n","# clf.fit(X_train, y_train)"],"id":"2e9df376","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fa08905","executionInfo":{"status":"ok","timestamp":1634113906941,"user_tz":-420,"elapsed":680,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"fe701b07-c2a8-4b88-e087-b147a92d1c75"},"source":["# Test\n","texts = [\n","    [\"mười hai tháng thì mỗi tháng anh chỉ cần trả bên em á là à một triệu rồi à một triệu một trăm sáu mươi hai\"],\n","    [\"cái lãi suất của chị rất thấp một triệu có mười bốn ngàn à nếu như mà lấy mười triệu thì mỗi tháng góp cho bên em có một triệu không trăm hai mươi tám ngàn à\"],\n","    [\"thì ủa thì em ví em ví dụ anh lấy mười triệu một tháng anh đóng có một triệu lẻ mấy chục ngàn à có gì đâu mà ớn\"],\n","    [\"thì gốc với lãi tới tháng một tháng là mình đóng là tầm khoảng là hai triệu năm trăm mười chín mười tám ngàn á chị\"],\n","    [\"dạ ý là bên công ty bên em mới gửi cái thông báo xuống hỗ trợ khách hàng trong cái dợt dịch này á chị nên em gọi lại cho chị dung ha\"],\n","    [\"hà thôi để chi coi lại coi chứ chị chưa có à trong mùa dịch này chị chưa có lấy được\"],\n","    [\"nói chung thì năm mươi sáu triệu mà mỗi tháng mình góp bên em á có hai triệu sáu trăm bảy ba ngàn á thì nó\"]\n","]\n","\n","## preprocessing (here we use tokenization with pyvi)\n","preprocessed_texts = [ViTokenizer.tokenize(text[0]) for text in texts]\n","for text in preprocessed_texts:\n","    print(clf.predict([text]), np.max(clf.predict_proba([text])))"],"id":"8fa08905","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-10-13 08:31:46,141 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,178 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,208 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,237 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,266 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,297 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,325 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,354 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,392 INFO: 1 batches, avg len: 3.0\n"]},{"output_type":"stream","name":"stdout","text":["[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.9994855446750753\n","[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.999462479855063\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.9610813804613358\n","[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.9130420881927251\n"]},{"output_type":"stream","name":"stderr","text":["2021-10-13 08:31:46,426 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,455 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,487 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,515 INFO: 1 batches, avg len: 3.0\n","2021-10-13 08:31:46,544 INFO: 1 batches, avg len: 3.0\n"]},{"output_type":"stream","name":"stdout","text":["[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.9999724985615515\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.9994329014869678\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 1]] 0.9999625960583886\n"]}]},{"cell_type":"code","metadata":{"id":"58940204"},"source":["vect = best_estimator[\"vect\"]\n","print(\"first 10 features in the vocabulary:\")\n","pprint([(k, v) for idx, (k, v) in enumerate(vect.vocabulary_.items())][:10])\n","print(f\"\\nNumber of features: {len(vect.get_feature_names_out())}\")"],"id":"58940204","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbaf03f4"},"source":["vect.stop_words_"],"id":"bbaf03f4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"1ac7ca1a"},"source":["pd.DataFrame.from_dict(gridcv.cv_results_)"],"id":"1ac7ca1a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2PEQTE1_fz73"},"source":["## Visualization"],"id":"2PEQTE1_fz73"},{"cell_type":"markdown","metadata":{"id":"9UmMFkxRf-b2"},"source":["### Learning Curve"],"id":"9UmMFkxRf-b2"},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"47c5733f","executionInfo":{"status":"ok","timestamp":1634103448987,"user_tz":-420,"elapsed":1909955,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"4c32148e-a5d6-4d1d-ed86-4c81a9128adb"},"source":["def plot_learning_curve(estimator, title, X, y, axe=None, ylim=None, cv=None,\n","                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n","    if axe is None:\n","        _, axes = plt.subplots(1, 1, figsize=(20, 5))\n","\n","    axe.set_title(title)\n","\n","    if ylim is not None:\n","        axe.set_ylim(*ylim)\n","    axe.set_xlabel(\"Training examples\")\n","    axe.set_ylabel(\"Score\")\n","\n","    train_sizes, train_scores, test_scores, fit_times, _ = \\\n","        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n","                       train_sizes=train_sizes, scoring=\"f1_micro\",\n","                       return_times=True)\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    fit_times_mean = np.mean(fit_times, axis=1)\n","    fit_times_std = np.std(fit_times, axis=1)\n","\n","    # Plot learning curve\n","    axe.grid()\n","    axe.fill_between(train_sizes, train_scores_mean - train_scores_std,\n","                         train_scores_mean + train_scores_std, alpha=0.1,\n","                         color=\"r\")\n","    axe.fill_between(train_sizes, test_scores_mean - test_scores_std,\n","                         test_scores_mean + test_scores_std, alpha=0.1,\n","                         color=\"g\")\n","    axe.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n","                 label=\"Training score\")\n","    axe.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n","                 label=\"Cross-validation score\")\n","    axe.legend(loc=\"best\")\n","\n","    return plt\n","\n","_, axe = plt.subplots(1, 1, figsize=(8, 8))\n","plot_learning_curve(clf, \"Learning Curve\", X, y,\n","                    axe=axe, ylim=(0.0, 1.01), n_jobs=1)\n","plt.show()"],"id":"47c5733f","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2021-10-13 05:05:38,984 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:05:38,989 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:05:39,015 INFO: char embedding size: 3920\n","2021-10-13 05:05:39,511 INFO: word embedding size: 88497\n","2021-10-13 05:05:46,727 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:05:47,609 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:06:00,539 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:06:05,795 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:06:07,772 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:06:07,779 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:06:07,817 INFO: char embedding size: 3920\n","2021-10-13 05:06:08,323 INFO: word embedding size: 88497\n","2021-10-13 05:06:15,360 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:06:16,526 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:06:52,255 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:06:57,583 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:07:03,838 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:07:03,841 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:07:03,877 INFO: char embedding size: 3920\n","2021-10-13 05:07:04,394 INFO: word embedding size: 88497\n","2021-10-13 05:07:11,463 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:07:12,910 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:08:03,668 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:08:09,322 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:08:20,141 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:08:20,142 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:08:20,180 INFO: char embedding size: 3920\n","2021-10-13 05:08:20,691 INFO: word embedding size: 88497\n","2021-10-13 05:08:27,778 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:08:29,513 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:09:37,698 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:09:43,665 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:09:58,999 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:09:59,001 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:09:59,039 INFO: char embedding size: 3920\n","2021-10-13 05:09:59,512 INFO: word embedding size: 88497\n","2021-10-13 05:10:06,348 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:10:08,353 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:11:28,935 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:11:35,246 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:11:55,278 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:11:55,281 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:11:55,318 INFO: char embedding size: 3920\n","2021-10-13 05:11:55,796 INFO: word embedding size: 88497\n","2021-10-13 05:12:02,685 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:12:03,589 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:12:15,939 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:12:21,050 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:12:23,046 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:12:23,048 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:12:23,086 INFO: char embedding size: 3920\n","2021-10-13 05:12:23,566 INFO: word embedding size: 88497\n","2021-10-13 05:12:30,456 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:12:31,598 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:13:12,117 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:13:17,693 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:13:24,239 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:13:24,247 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:13:24,279 INFO: char embedding size: 3920\n","2021-10-13 05:13:24,779 INFO: word embedding size: 88497\n","2021-10-13 05:13:31,626 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:13:33,085 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:14:28,735 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:14:34,535 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:14:45,473 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:14:45,482 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:14:45,518 INFO: char embedding size: 3920\n","2021-10-13 05:14:46,005 INFO: word embedding size: 88497\n","2021-10-13 05:14:52,807 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:14:54,566 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:16:03,888 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:16:09,962 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:16:25,609 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:16:25,611 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:16:25,650 INFO: char embedding size: 3920\n","2021-10-13 05:16:26,161 INFO: word embedding size: 88497\n","2021-10-13 05:16:33,029 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:16:35,022 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:17:58,389 INFO: 1 batches, avg len: 1236.0\n","2021-10-13 05:18:04,613 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:18:24,590 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:18:24,592 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:18:24,632 INFO: char embedding size: 3920\n","2021-10-13 05:18:25,098 INFO: word embedding size: 88497\n","2021-10-13 05:18:31,842 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:18:32,702 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:18:44,897 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:18:49,979 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:18:51,969 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:18:51,971 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:18:52,009 INFO: char embedding size: 3920\n","2021-10-13 05:18:52,492 INFO: word embedding size: 88497\n","2021-10-13 05:18:59,242 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:19:00,366 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:19:40,608 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:19:46,158 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:19:52,549 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:19:52,551 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:19:52,591 INFO: char embedding size: 3920\n","2021-10-13 05:19:53,106 INFO: word embedding size: 88497\n","2021-10-13 05:19:59,834 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:20:01,302 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:20:55,522 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:21:01,190 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:21:11,946 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:21:11,957 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:21:11,993 INFO: char embedding size: 3920\n","2021-10-13 05:21:12,475 INFO: word embedding size: 88497\n","2021-10-13 05:21:19,144 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:21:20,846 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:22:28,655 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:22:34,604 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:22:50,057 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:22:50,060 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:22:50,106 INFO: char embedding size: 3920\n","2021-10-13 05:22:50,581 INFO: word embedding size: 88497\n","2021-10-13 05:22:57,151 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:22:59,175 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:24:22,429 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:24:28,718 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:24:48,585 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:24:48,589 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:24:48,626 INFO: char embedding size: 3920\n","2021-10-13 05:24:49,116 INFO: word embedding size: 88497\n","2021-10-13 05:24:55,698 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:24:56,581 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:25:08,926 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:25:14,044 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:25:16,039 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:25:16,047 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:25:16,084 INFO: char embedding size: 3920\n","2021-10-13 05:25:16,577 INFO: word embedding size: 88497\n","2021-10-13 05:25:23,184 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:25:24,369 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:26:04,232 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:26:09,667 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:26:16,096 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:26:16,098 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:26:16,139 INFO: char embedding size: 3920\n","2021-10-13 05:26:16,635 INFO: word embedding size: 88497\n","2021-10-13 05:26:23,213 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:26:24,653 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:27:18,349 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:27:23,919 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:27:34,674 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:27:34,682 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:27:34,715 INFO: char embedding size: 3920\n","2021-10-13 05:27:35,230 INFO: word embedding size: 88497\n","2021-10-13 05:27:41,807 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:27:43,503 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:28:49,669 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:28:55,606 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:29:10,810 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:29:10,813 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:29:10,845 INFO: char embedding size: 3920\n","2021-10-13 05:29:11,343 INFO: word embedding size: 88497\n","2021-10-13 05:29:17,977 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:29:19,965 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:30:40,084 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:30:46,382 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:31:06,183 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:31:06,193 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:31:06,229 INFO: char embedding size: 3920\n","2021-10-13 05:31:06,724 INFO: word embedding size: 88497\n","2021-10-13 05:31:13,368 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:31:14,248 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:31:26,359 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:31:31,379 INFO: 1 batches, avg len: 495.0\n","2021-10-13 05:31:33,389 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:31:33,393 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:31:33,433 INFO: char embedding size: 3920\n","2021-10-13 05:31:33,915 INFO: word embedding size: 88497\n","2021-10-13 05:31:40,539 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:31:41,679 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:32:21,272 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:32:26,561 INFO: 1 batches, avg len: 1605.0\n","2021-10-13 05:32:32,825 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:32:32,833 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:32:32,868 INFO: char embedding size: 3920\n","2021-10-13 05:32:33,377 INFO: word embedding size: 88497\n","2021-10-13 05:32:39,966 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:32:41,395 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:33:35,021 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:33:40,680 INFO: 1 batches, avg len: 2715.0\n","2021-10-13 05:33:51,489 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:33:51,491 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:33:51,539 INFO: char embedding size: 3920\n","2021-10-13 05:33:52,016 INFO: word embedding size: 88497\n","2021-10-13 05:33:58,629 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:34:00,360 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:35:07,921 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:35:13,984 INFO: 1 batches, avg len: 3825.0\n","2021-10-13 05:35:29,489 WARNING: Could not find config.  Trying pretrained_embs/ELMo/vi/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:35:29,492 WARNING: Could not find config.  Trying /content/drive/My Drive/Intent-Detection/pretrained_embs/ELMo/ELMoForManyLangs/elmoformanylangs/configs/cnn_50_100_512_4096_sample.json\n","2021-10-13 05:35:29,531 INFO: char embedding size: 3920\n","2021-10-13 05:35:30,016 INFO: word embedding size: 88497\n","2021-10-13 05:35:36,656 INFO: Model(\n","  (token_embedder): ConvTokenEmbedder(\n","    (word_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(88497, 100, padding_idx=3)\n","    )\n","    (char_emb_layer): EmbeddingLayer(\n","      (embedding): Embedding(3920, 50, padding_idx=3917)\n","    )\n","    (convolutions): ModuleList(\n","      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n","      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n","      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n","      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n","      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n","      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n","      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n","    )\n","    (highways): Highway(\n","      (_layers): ModuleList(\n","        (0): Linear(in_features=2048, out_features=4096, bias=True)\n","        (1): Linear(in_features=2048, out_features=4096, bias=True)\n","      )\n","    )\n","    (projection): Linear(in_features=2148, out_features=512, bias=True)\n","  )\n","  (encoder): ElmobiLm(\n","    (forward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_0): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (forward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","    (backward_layer_1): LstmCellWithProjection(\n","      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n","      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n","      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n","    )\n","  )\n",")\n","2021-10-13 05:35:38,641 INFO: 1 batches, avg len: 4935.0\n","2021-10-13 05:37:01,424 INFO: 1 batches, avg len: 1235.0\n","2021-10-13 05:37:07,701 INFO: 1 batches, avg len: 4935.0\n"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAfgAAAHwCAYAAABKe30SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5xUdf3H8ddn77uwchfkIpfCVJSLImiGoiiglYhm3lLUErXEW2mYpmSiZnn9aSmZ11Q0MkMlEdNNDDVQkUQTkUDBvIHA7sLev78/vnN2Z4fZ3dndmdndw/vZYx47c+acmTNH832+d3POISIiIuGS0dYnICIiIsmngBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREFLAi8gOzGycmb3X1uchIi2ngBdpZ8xsrZkd0Zbn4Jxb7Jz7Wqo+38wmmdlLZlZsZp+b2T/M7JhUfZ/IzkgBL7ITMrPMNvzu7wB/Ah4E+gO9gauAb7fgs8zM9N8xkTj0fwyRDsLMMsxsppl9YGYbzexxM+se9f6fzOwTM9sSKR0Pi3rvfjP7nZktMLNS4LBITcFPzGxF5JjHzCwvsv94M1sfdXyD+0bev8zM/mdmH5vZD8zMmdlX4/wGA24Gfumcu8c5t8U5V+Oc+4dz7uzIPrPM7I9RxwyKfF5W5HWRmc02s38C24BLzWxZzPdcbGbzI89zzew3ZvahmX1qZneZWX4r/3GItHsKeJGOYwZwLHAo0Bf4Ergz6v2/AUOBXYE3gIdjjj8FmA0UAi9Htn0XmAwMBoYDZzTy/XH3NbPJwCXAEcBXgfGNfMbXgAHAvEb2ScRpwHT8b7kL+JqZDY16/xTgkcjzG4A9gJGR8+uHrzEQCTUFvEjHcS5whXNuvXOuHJgFfCco2Trn7nXOFUe9N8LMukQd/1fn3D8jJeayyLbbnXMfO+c2AU/hQ7AhDe37XeA+59xK59y2yHc3pEfk7/8S/dENuD/yfVXOuS3AX4GTASJBvycwP1JjMB242Dm3yTlXDFwHnNTK7xdp9xTwIh3HQOAvZrbZzDYD7wLVQG8zyzSzGyLV91uBtZFjekYd/1Gcz/wk6vk2oHMj39/Qvn1jPjve9wQ2Rv7u1sg+iYj9jkeIBDy+9P5k5GajF1AAvB513Z6NbBcJNQW8SMfxEXCUc65r1CPPObcBH2pT8NXkXYBBkWMs6vhULR35P3xnucCARvZ9D/87jm9kn1J8KAf6xNkn9rcsAnqZ2Uh80AfV818A24FhUdesi3OusRsZkVBQwIu0T9lmlhf1yMK3Nc82s4EAZtbLzKZE9i8EyvEl5AJ8NXS6PA6caWZ7mVkB8POGdnR+fepLgJ+b2Zlmtkuk8+A3zGxOZLflwCFmtnukieHypk7AOVeJ75n/a6A7PvBxztUAvwduMbNdAcysn5lNavGvFekgFPAi7dMCfMkzeMwCbgPmA8+ZWTHwKjA2sv+DwDpgA/BO5L20cM79DbgdeBFYHfXd5Q3sPw84ETgL+Bj4FLgW346Oc24R8BiwAngdeDrBU3kEX4PxJ+dcVdT2nwbnFWm+eB7f2U8k1MzfUIuIJIeZ7QW8DeTGBK2IpJFK8CLSamY2NTLevBvwK+AphbtI21LAi0gynAN8BnyA79l/Xtuejoioil5ERCSEVIIXEREJIQW8iIhICGW19QkkS8+ePd2gQYPa+jTahdLSUjp16tTWp7FT0LVOL13v9NG1Tq+WXu/XX3/9C+dc3JkZQxPwgwYNYtmyZU3vuBMoKipi/PjxbX0aOwVd6/TS9U4fXev0aun1NrN1Db2nKnoREZEQUsCLiIiEkAJeREQkhELTBi8i0l5VVlayfv16ysrK2vpUEtalSxfefffdtj6NnUZT1zsvL4/+/fuTnZ2d8Gcq4EVEUmz9+vUUFhYyaNAgzKzpA9qB4uJiCgsL2/o0dhqNXW/nHBs3bmT9+vUMHjw44c9UFb2ISIqVlZXRo0ePDhPu0r6YGT169Gh2DZACXkQkDRTu0hot+fdHAS8iEnIbN25k5MiRjBw5kj59+tCvX7/a1xUVFY0eu2zZMi644IImv+PrX/96sk5XkiRlAW9m95rZZ2b2dgPvm5ndbmarzWyFme0X9d40M3s/8piWqnMUEWmXHn4YBg2CjAz/9+GHW/VxPXr0YPny5Sxfvpxzzz2Xiy++uPZ1Tk4OVVUNr+w7evRobr/99ia/Y8mSJa06x1Rp7LeFXSpL8PcDkxt5/yhgaOQxHfgdgJl1B64GxgJjgKsja0yLiITfww/D9Omwbh045/9On97qkI91xhlncO655zJ27Fguu+wy/vWvf3HQQQcxatQovv71r/P+++8Dfoa1b33rWwDMmjWLs846i/HjxzNkyJB6wd+5c+fa/cePH893vvMd9txzT0499VSCVUsXLFjAnnvuyf77788FF1xQ+7nRVq5cyZgxYxg5ciTDhw+vPY8HH3yQ4cOHM2LECE477TQA1q5dy+GHH87w4cOZMGECH374Ydzf9sEHHzB58mT2339/xo0bx3/+85+kXsv2KmW96J1zL5nZoEZ2mQI86Pw/+VfNrKuZ7QaMBxY55zYBmNki/I3Co6k6VxGRtLnoIli+vOH3X30Vysvrb9u2Db7/ffj97+MfM3Ik3Hprs09l/fr1LFmyhMzMTLZu3crixYvJysri+eef5xe/+AV//etfdzjmP//5Dy+++CLFxcV87Wtf47zzztth6Nabb77JypUr6du3LwcffDD//Oc/GT16NOeccw4vvfQSgwcP5uSTT457TnfddRcXXnghp556KhUVFVRXV7Ny5UquvfZalixZQs+ePdm0aRMAM2bMYNq0aUybNo17772XCy64gCeffHKH3zZhwgTuuusuhg4dymuvvcYPf/hDXnjhhWZfr46mLYfJ9QM+inq9PrKtoe0iIuEXG+5NbW+FE044gczMTAC2bNnCtGnTeP/99zEzyhv4vm9+85vk5uaSm5vLrrvuyqeffkr//v3r7TNmzJjabSNHjmTt2rV07tyZIUOG1A7zOvnkk5kzZ84On3/QQQcxe/Zs1q9fz3HHHcfQoUN54YUXOOGEE+jZsycA3bt3B+CVV17hiSeeAOC0007jsssu2+G3lZSUsGTJEk444YTa9xr6bWHTocfBm9l0fPU+vXv3pqioqG1PqJ0oKSnRtUgTXev06qjXu0uXLhQXF/sXv/xlo/t2GjaMjI8+2mF7zYABlD71VMMHBp/fhPLycrKzs6msrCQjI6P2vGbOnMlBBx3Egw8+yLp16zj66KMpLi5m27ZtVFVVUVxcXHtscIyZsXnzZrp06RI5Bb9/ZmZm7T7V1dWUlJRQWlpKdXV17fbt27fXfm60b3/72wwbNoyFCxcyefJkbrvtNsrKyqioqNhhX+ccxcXFtb8neB3927Zu3UqXLl1YvHhxzOVK7HqlS/S1aUhZWVmz/v1vy4DfAAyIet0/sm0Dvpo+entRvA9wzs0B5gCMHj3aJW3lo4cfhiuugA8/hN13h9mz4dRTk/PZaaBVoNJH1zq9Our1fvfddxOfNOb6632b+7ZtddsKCsi4/vqkTDwTlL6zs7PJz8+v/cxt27bxla98hcLCQubNm4eZUVhYSEFBAVlZWRQWFtYeGxyTkZFB586da1/H7g+Qk5NDXl4e++23H+vWrWPjxo0MGjSI+fPn19svsGbNmtq29s8++4zVq1dz9NFHM3XqVGbOnEmPHj3YtGkT3bt35+CDD+aZZ57htNNO4/777+eQQw6hsLCw3m8rLCxkyJAhPPvss5xwwgk451ixYgUjRoxo9bVMpkQmFsrLy2PUqFEJf2ZbDpObD5we6U1/ILDFOfc/YCEw0cy6RTrXTYxsS480dXAREYnr1FNhzhwYOBDM/N85c1JeyLjsssu4/PLLGTVqVEp6nufn5/Pb3/62trNbYWFhbck/2uOPP84+++zDyJEjefvttzn99NMZNmwYV1xxBYceeigjRozgkksuAeD//u//uO+++xg+fDgPPfQQt912W9zvfvjhh/nDH/7AiBEjGDZsWNy+BWFkQe/GpH+w2aP4knhP4FN8z/hsAOfcXeZH7d+B70C3DTjTObcscuxZwM8iHzXbOXdfU983evRol5T14AcN8qEeq3Nn38klNxdycvzf2EdODuTl7fg8+Bv9yM72Q2ACwSQG0ZMZNLatkfeLFi9m/LhxTR8vrdZRS5QdVUe93u+++y577bVXW59Gs6RiqtqSkhI6d+6Mc44f/ehHDB06lIsvvjip39FRJXK94/17ZGavO+dGx9s/lb3o43eRrHvfAT9q4L17gXtTcV5Nigyz2EFJCfzf/0FNTXK+x8yHfE5O/UdwcxDvvehHdnb9fYPnOTn03rzZ99Jt7PjYm5HcXH/DEX0DENyAmNVtj74pifc80W2xz5u6wZk7F66+GtavhwED4LrrOlSziYjA73//ex544AEqKioYNWoU55xzTlufUqh16E52KbH77vFL8H37wosvQlUVVFTUPcrL/d/Kyrq/wSN4P/p19L7B8+AzYj+zogJKS2HTpobfr6zc4VRbXE5o6qYiqHkI/jZ1AxF9MxJ7YxJvW+xxWZF/PefPh5//HIJ5mD/8EH7wA/jySzjttLrPyNK/ziLt2cUXX6wSexrpv4ixZs+O28GFG26AIUN8u3zso6am/vNEHtH7BoLmkqaq0YMStZk/PvoGorKS1957j7EDBtS/kYh3c9DU6/LyHd/fvh22bGn8+GTJyPDhXV5ed20CZWW+M9KkSXXXPjsb8vP9P6/gJiEyBEhEZGejgI8VVPumsxd9vBuFxh6N3ThUV7O9pMTfjMTeQDRH9E1EIo/o3xJ9YxF7wxB7M9DU+xUVcPfd8c/x44/hjjvg6KNhn338d5eV+eFCzvnzysqqC/2g5kChLyI7AQV8PKeemt723SAkM5I0qGHNGh/wgURuGmIf1dX+0ciNRNxaiGhBKTpezUTszUFjNw9PPw0bNuz4+bm5PuBvvx3694ejjvJhv//+dSFeU+NrHUpK6pf0CwrqSvoKfREJIQX8ziC2lJ0KTTVdNKcWIvYm4uKL4cor69rgwZfKb7wRDj0UFi2CBQvggQf8VJ69evmq+6OPhoMO8p0Jo1VX+yaY4mL/PUGHx9jQT9YNl4hIG1DAS3Kk8ibixz+GPn3gZz+Djz7ypfWLL4YjjvCl+JNO8o/iYnjhBR/2TzwBf/wjdOni9zvqKBg/3t8YZGbuWGKvrval/C1b6rZlZ0OnTj70s7MV+tKhffLJJ1x00UUsXbqUrl270rt3b2699Vb22GOPtj61eu6//36WLVvGHXfcwV133UVBQQGnn356vX3Wrl3Lt771Ld5+O+5ipbX7LFmyhFNOOQXwy94++OCDCa2MFxYKeOkYYptNnPNV75s2wdatvq29c2eYMsU/tm+HxYt92C9aBH/+sy/JH364D/sjjoBddqn7vMxMH/7Rqqr8TcPmzXXNDLm5PvTz833gi6TAw/9+mCv+fgUfbvmQ3bvszuwJszl135Y3GzrnmDp1KtOmTWPu3LkAvPXWW3z66af1Ar6qqoqsdjQa5dxzz23xsWvXruWRRx6pDfjRo0czenTc4eJtKpXXXMUR6ZjMfMm6f38YPNiHdWmpL4VXV/sAnjjRr7C1fDk8+ih897uwbBnMmAHDh/shdo88Al98Ef87gg56nTtDYaF/ZGT4G4oNG/xwyvJy//fzz/13V1Ts2ONfpBke/vfDTH9qOuu2rMPhWLdlHdOfms7D/275bJovvvgi2dnZ9QJzxIgRjBs3jqKiIsaNG8cxxxzD3nvvTVlZGWeeeSYHHnggo0aN4sUXXwTiL+NaWlrKN7/5TUaMGME+++zDY489Vu97a2pqGDRoEJs3b67dNnToUD799FOeeuopxo4dy6hRozjiiCP49NNPdzjvWbNm8Zvf/AaA119/nREjRjBixAjuvPPO2n3Wrl3LuHHj2G+//dhvv/1q16WfOXMmixcvZuTIkdxyyy31lr3dtGkTxx57LMOHD+fAAw9kxYoVtd/X0HK4gerqas444wz22Wcf9t13X2655RYAVq9ezRFHHMGIESPYb7/9+OCDD3DOcemll9buG1yf2GteXV3NlVdeyQEHHMDw4cO5u6GOxc3Ufm7VRFoqNxd23RW6d/cl7k2bfOk7eiKgQw7xj9mz4fXX4W9/849LL4Wf/hTGjPEl+6OOgn6NLF6YlVV/vH0wOdDWrX5cfiAvz5f08/Lqqvc1i6AAFz17Ecs/aXi52FfXv0p5df3hptsqt/H9v36f378ef7nYkX1GcuvkhpeLffvtt9l///0bfP+NN97g7bffZvDgwdx0002YGa+++iobNmxg4sSJrFq1Ku4yrgsWLKBv374888wzgF+RLlpGRgZTpkzhL3/5C2eeeSavvfYaAwcOpHfv3nzjG9/g1Vdfxcy45557uPHGG7npppsaPMczzzyTO+64g0MOOYRLL720dvuuu+7KokWLyMvL4/333+fkk09m2bJl3HDDDfzmN7/h6aefBqi3SMvVV1/NqFGjePLJJ3nhhRc4/fTTWR5Zwrep5XCXL1/Ohg0bapsHgpuXU089lZkzZzJ16lTKysqoqanhiSeeYPny5bz11lt88cUXHHDAARxyyCE7XPM5c+awyy67sHTpUsrLyzn44IOZOHFi7cp7LaUSvIRHVhZ06+ZL9EFIFxf76vqgVJ2RAQccAFddBUuWwMKFcMEFPpyvvtoH/dFH+1kLV69O/HuDkn7nzj7YnfNV+xs2wH//6z/ro49g40Zf01BZqZK+xBUb7k1tT4YxY8bUhsnLL7/M9773PQD23HNPBg4cyKpVqzjooIO47rrr+NWvfsW6devIz89n3333ZdGiRfz0pz9l8eLFceeWP/HEE2tLrnPnzuXEE08E/HrtkyZNYt999+XXv/41K1eubPD8Nm/ezObNm2vD8bTTTqt9r7KykrPPPpt9992XE044gXfeeafJ3/vyyy/Xfsbhhx/Oxo0b2bp1K1C3HG7Pnj1rl8ONNmTIENasWcOMGTN49tln2WWXXSguLmbDhg1MnToV8IvCFBQU8PLLL3PyySeTmZlJ7969OfTQQ1m6dOkO1/y5557j0UcfZeTIkYwdO5aNGzfy/vvvN/k7mqISvIRPRkZd2JaV+fDeutW3s+fl1Z+Cd599/OPSS30IP/usf9xwg3/ssUfd8LthwxIrhQe98qPb6J3ztQpfflk3rNBsx5J+VpZK+iHXWEkbYNCtg1i3ZcfZNAd2GUjRGUUt+s5hw4Yxb968Bt/v1KlTk59xyimnMHbsWJ555hmOPvpo7r77bg4//HDeeOMNFixYwJVXXsmECROYNGlS7RS011xzDd/+9rdZvXo1n3/+OU8++SRXXnklADNmzOCSSy7hmGOOoaioiFmzZrXot91yyy307t2bt956i5qaGvJiR800U25ubu3zzMzMHRbe6datG2+99RYLFy7krrvu4vHHH29wkZvGRF9z5xy//vWva28QkkUleAm3vDzYbTc/L0C3bnVj4uOtlvXVr8L55/tx9//6l1+3u0cPX5qfNMkPufvFL2Dp0uZPIBQ9FC+4+Sgo8P0FNm3yc+yvWeNvMjZs8DcC27bFnYpYwm32hNkUZBfU21aQXcDsCbNb/JmHH3445eXlzJkzp3bbihUrdlgjHWDcuHE8HFk9c9WqVXz44Yd87WtfY82aNQwZMoQLLriAKVOmsGLFCj7++GMKCgr43ve+x6WXXsobb7zB2LFjWb58OcuXL+eYY47BzJg6dSqXXHIJe+21Fz169AB8dX6/SE3bAw880Oj5d+3ala5du/Lyyy8D1J5f8Dm77bYbGRkZPPTQQ1RXVwN+6dqG1leP/o1FRUX07NmTXaI73Tbiiy++oKamhuOPP55rr72WN954g8LCQvr378+TTz4JQHl5Odu2bWPcuHE89thjVFdX8/nnn/PSSy8xZsyYHT5z0qRJ/OEPf6Ay8v/3VatWUVpamtD5NEYleNk5ZGf7sO7WzQf8F1/4AA1W94vVrx+cdZZ/bNwIzz3ne+Tfd59funPXXWHSJLrtvbcv5bekR71Z3WRAgWAmwI0bffgHEyAFs/FFl/QllILe8snsRW9m/OUvf+Giiy7iV7/6FXl5eQwaNIhbb72VDTGTSP3whz/kvPPO48ADDyQnJ4f777+f3NxcHn/8cR566CGys7Pp06cPP/vZz1i6dCmXXnopGRkZZGdn87vf/S7u95944okccMAB3H///bXbZs2axQknnEC3bt04/PDD+e9//9vob7jvvvs466yzMDMmTpxY73yPP/54HnzwQSZPnlxbMh4+fDiZmZmMGDGCM844o9466kFnuuHDh1NQUNDkDUa0DRs2cOaZZ1ITucm//vrrAXjooYc455xzuOqqq8jOzuZPf/oTU6dO5ZVXXmHEiBGYGTfeeCN9+vThP//5T73P/MEPfsCqVavYb7/9cM7Rq1ev2puF1kjZcrHplrTlYkOgoy6pmVbRw+xKS+uGyTVVPb51a91Y+xde8J/Rtasfdnf00b4jX+xwu9YK1huoqqrfl6CgwFfv70SL7XTUf7e1XKw0pUMtFyvSrgXD7AoK/FC3LVt8pziomwwnnl12gWOP9Y/t2/n3H//Ivm+/7cfaz5vnP++ww3zYT5jgh9a1VkbGjjUNNTX+vEtL65oLtNiOiERRwIs0NcyuIfn5bPz61+Hss30J+5VXfMn+2WfhmWf8sd/4hg/7iRN9E0GyNBT6WmxHRCIU8CKBYJhdly6+6v3zz31YZmX5tu/Gqu9jx9q/8YYP+7/9DX7yEx/IY8f6sJ88Gfr2Tf75xwv96ur4i+0Es/FpsR2R0FLAi8TKyPAB2KlT48PsGpKZ6cfaB+PtV66sC/uf/9w/Ro2qm1gneuW/ZGto3v3SUt8sEZT0tdhOyjnnMA2BlBZqSX85BbxIY4Jhdj171s1WV1PjtyfSqS16rP1ll/lhcMEsetdd5x977lkX9nvvnfpx8FpsJ+3y8vLYuHEjPXr0UMhLsznn2LhxY7PH+CvgRRLR0DC75t5Vf/Wrfi78GTP8ePcg7G+9FW65BQYOrAv7/fZLX6AmsthOMKwverGdnBxNzJOA/v37s379ej7//PO2PpWElZWVtXrSGElcU9c7Ly+P/v37N+szFfAizZGR4XvSFxb6tu2PPvIhmOgwu2j9+sEPfuAfn3/ux9r/7W/whz/AXXdB796+vf6oo+DAA9O/el3svPvgQz923v3cXH9NunRRCb8B2dnZrZ5XPN2KiorqjR2X1ErF9VbAi7REMMwuO9vPfb95c2LD7BrSq1fdkrhbt8Lzz/uwf+wxeOABP9Z+4kQf9occ4psI2kK80K+s9Dcomzb50QidO6tUL9IOKOBFWisnxwdbjx4+nBMdZteQXXaB447zj+3boajIh/2zz8Ljj/sq8mBd+wkTfKC2paB9vqrKNzvk5fnah2RP+CMizaKAF0mWzEzfRt+1q2+f/+KLxIfZNSQ/v65NvqKi/lj7p57yNxHjxvnhd0ce6cfyt5WsLH9zUlEBH37obzx69WrZTY6ItJoCXiTZzOoPs9u82fdOT3SYXUNycuDQQ/3juuv8uvbB8Lvnn/eff+CBPuwnTfK9/9tCMIve9u1+AZ3u3f1jJ5hKV6Q9UY8YkVTKy4M+fepWswtmmou3ml1zZGb6tetnzYJXX/Ul+h/9CD77DK64AkaPhm9/G373O78efVvIz/edEbdu9UEfvVSuiKScAl4kHYJhdkOG+JJ1MAStrKz1n20G++4LP/2pb68vKvLPq6rg2mv9dLlHHAE33wzvvtv8oX2tPbdgAp3PP/c3G8FUuiKSUqozE0mn2GF2mza1fJhdQ4YO9Y8LLvDrzAdj7W++GW66CQYN8tX4Rx0FI0emZ2hbRoZvk1dHPJG0UcCLtIXokm1FReuH2TWkf3+/GM7ZZ/sS9MKFPuznzIHf/tY3HwSd+MaOTX07eXRHvHXr/I2OOuKJpIQCXqStRQ+zKy6GjRtbN8yuIb16wfe+5x9bttSNtX/0UbjvPt9HYNIkH/bjxtVftCbZ1BFPJOX0/yaR9iIz0w+x69LFD7PbuNF3UMvObvkwu4Z06QLHH+8f27bVjbV/5hmYO9dXp0+Y4MP+8MP9iIBUyM/3v23rVl+D0auXZsQTSRIFvEh7k6phdg0pKPBt8kcf7avO//nPuol1/vpXX5I/9FAf9kce6Uv6gSeegBtugI8/9kvgzpzpJ+hp7u8tKPA97DUjnkjSKOBF2rNgmF1Qfb9pk1/5LT8/NdXZOTlw2GH+cf31sHRp3cQ6zz3nbzK+/nUf9jU1MHu2r2YH33nussv88+aGPKgjnkiSKeBFOoLsbN9G3bVr3Wp227fXVd+nQjBxzoEHwi9+Af/+tw/7BQvgZz+Lf8z27b5E35KAD6gjnkhSKOBFOpLoYXZlZb6dPtnD7OIxg+HD/WPmTHj/fRg/Pv6+H3+cnO+M1xFPRBKmniwiHZGZD/T+/f1qdrvsAqWlvnRfXZ367x861C93G09Bga9iT5boGfHKyzUjnkiCFPAiHV0wzO4rX/F/Kyp8qb6iIrXfO3Pmju3jmZm+xH3wwb49ft265HxX0BEvI0Mz4okkSAEvEhbBMLshQ3zJPiPDl3q3b09NEB53HNx4oy/Jm/m/t97q58Y/9VSYN8+Pp7/wQli9Onnf27mz73uwYYO/gQg6+YlIPWqDFwmbeMPstm6tq9ZP5jC7YN36WLNnw4wZcPfd8NBD8Oc/+8VvLrgA9tqr9d+rjngiTVIJXiTMgmF2gwf7oXbJWs0uEX36wNVXw2uv+ZXu/v53v+jN978PK1Yk5ztycnzQl5X5avvPPkvPbxPpABTwIjuDYJhdsJpddXXyVrNrSo8ecPnlPugvuQReecWPoz/tNFi2LDnfkZ/vq+61NK1ILQW8yM4kGGY3aBDsvrsvARcX++lqU91hrVs3+PGPfdDPnAnLl8OUKXDiibBkSeu/X0vTitSjgBfZGbXlMLvCQt8+/9prcNVVsGoVnHCCb8v/xz9aH8jBjHjqiCc7OQW8yM4uephd796+49rWrakfZnrNWFAAACAASURBVFdQAOec40vv114LH30Ep5ziO+M991zrgz7oiAc+5DdsSP1vEmlHFPAi4mVm+pXchgyBAQNSP8wukJ8PZ57pg/7GG/3sfGeeCRMnwtNPt74tXR3xZCelgBeR+oJhdrvv7tvqO3Xy1felpantuJaT48fPv/SSH09fVuZL+Icf7leta20oqyOe7GQU8CLSsIaG2VVWpu47s7N9m3xREfz2t75mYcYMOPRQ+jz7bOu+Wx3xZCeigBeRpsUOs6upSf0wu8xM38t+0SK45x4oLGTPm2+Gb3wDHnzQz0vfUuqIJzsBBbyIJK6xYXapqu7OyPDj5v/2N1Zce63vEHj55X5d+nvuaV0wqyOehJgCXkSaL3aYXdeuPmhLSlJXfW/GpjFjYP58mDvXf+/VV/v16n/7W//dLRXbEe/zz9URTzo8BbyItE5ODvTs6avv+/TxJflU9r4384vYzJvnO98NG+bnvh871nfO27Kl5Z8ddMTbskUd8aTDU8CLSHJkZtZV3w8c6DuyBb3vUzV5ztix8MgjfjjdAQfAr3/tt914I2za1LLPVEc8CQkFvIgkV1B9v9tuvlTfs2fdGvWt6RjXmFGj4P77YeFCOOQQuP12H/TXXutDuiXUEU86OAW8iKROVpafgz5Yoz4z01ffp2ru+332gTlz4IUXYNIkv1ztgQf6KXH/97+WfaY64kkHpYAXkdSLnjwnmPt+2zbfMS4Vndn22APuuMPPbT9lCjzwgO91/9Of+ilxW0Id8aSDUcCLSHrl5vqhbkGnvKqq1I2pHzIEbr4ZXn7Zr1r3+ON+HP0ll/hOdC0R3RHvv/9VRzxptxTwItI2gk55gwf7kn1eng/6VEyJO2AA3HAD/POfMG0a/PWvcOihcP758N57zf+8oCNefr464km7pYAXkbYVdMrr29eXuHv08J3xiouT39bdty9ccw28+qqf537hQj/X/dlnw9tvN//zojviffyxOuJJu6KAF5H2I5gSd/Bg3ykvI8MHfbLH1PfqBVde6dekv/BCWLzYd8qbNg3efLP5n5eV5de5B3XEk3ZDAS8i7U9GRv0V7QoL66ruk9mxrXt3uOwyH/SXXgrLlsG3vgUnn+y3NZc64kk7ooAXkfYt6JT3la/4knIqOuV16QIXXQT/+pcv2b/zDhx3HBx/vF++trm1B+qIJ+1ASgPezCab2XtmttrMZsZ5f6CZ/d3MVphZkZn1j3qv2syWRx7zU3meItIBZGb6R2ynvGQudNOpE5x3nm+jv+YaWLvWl+aPOQaef755Qa+OeNLGUhbwZpYJ3AkcBewNnGxme8fs9hvgQefccOAa4Pqo97Y750ZGHsek6jxFpIOJ7ZTXvXvdOvXJavfOz4fvfx+WLIHrr4fPPvPt85FV7Zp1Q6GOeNJGUlmCHwOsds6tcc5VAHOBKTH77A28EHn+Ypz3RUQaFr1Ofb9+PvyT2SkvNxdOP92Po7/5Zv/ZP/gBHHmkH2rXnDn21RFP0iyVAd8PiJ4yan1kW7S3gOMiz6cChWbWI/I6z8yWmdmrZnZsCs9TRDq6oJQ8cGD9TnnJmikvO9tPlPOPf/gZ8mpq4Ic/hPHj/eQ5zVkiVx3xJE3Mpag9yMy+A0x2zv0g8vo0YKxz7vyoffoCdwCDgZeA44F9nHObzayfc26DmQ3Bl/InOOc+iPmO6cB0gN69e+8/d+7clPyWjqakpITOnTu39WnsFHSt06vZ17u62j9qavxNgFlyTqSmhl4vv8zARx6h85o1bO/Thw9PPJFPjjwSl5PT7M8CfAk/MzM555cE+nc7vVp6vQ877LDXnXOj472XyoA/CJjlnJsUeX05gHPu+gb27wz8xznXP8579wNPO+fmNfR9o0ePdsuWLUvGqXd4RUVFjB8/vq1PY6ega51eLbrezvkq+02bfKk+I8O3sWckoQLTOVi0CG67DZYv9yvo/ehHcNJJ/jsSVVPjOwtmZvoRA507J+9mpIX073Z6tfR6m1mDAZ/KKvqlwFAzG2xmOcBJQL3e8GbW08yCc7gcuDeyvZuZ5Qb7AAcD76TwXEUkrILe7P37+x740Z3ymlO13tBnT5zo16N/5BE/Je6VV/qFbe66y99QJEId8SQFUhbwzrkq4HxgIfAu8LhzbqWZXWNmQa/48cB7ZrYK6A3MjmzfC1hmZm/hO9/d4JxTwItI6+Tk+KlwhwzxvfCdS06nPDM/t/1f/gLz5vnV7H75S78m/e23++9IRGxHvI8/Vkc8abGsVH64c24BsCBm21VRz+cBO1S7O+eWAPum8txEZCeWkeGDtLDQl+Y3b/br1IOvWm9NW/hBB/nHsmW+6v5Xv/Kl+bPO8kPvunVr+jNycvxj+3bfEa97d39cVkr/ky0ho5nsRGTnlpfnl60dMsS3f1dW+hJ3eXnrPnf0aHjoIXj2WV9lf8stvkR/3XXwxReJfYZmxJNWUMCLiIAvHXft6tvpBwzw7eHJWL52333hnnv8THhHHAG//a0P+lmz4JNPmj5eM+JJCyngRUSixXbK69bNV5W3tlPeXnv5cC8q8gva3HuvL9n/7Gewfn3Tx6sjnjSTAl5EpCE5OdCzp1/oZrfdktMp76tf9W3zixfDd77je98ffDD85Cd+7vumqCOeJEgBLyLSlIwMP/vcoEF+trxOnfwseSUlzZuuNtrAgXDjjfDPf8Jpp8ETT8C4cTBjBqxe3fTxwYx4QUc8zYgnMRTwIiLNEXTK+8pXoFcvX3puTae8fv3g2mvhlVfg7LP9Yjbjx8O55/pla5uijnjSAAW8iEhLZGX59vkhQ3x7fVZW65av7d0brroKXnvNz4b34ot+UZuzzoK33mr82IY64lVUtLyGQTo8DaoUEWkNM19l36mTD9StW30p2jm/Gl12dvM+r0cPuPxyvy79vff6HvgLF8Jhh8GFF8IBBzR8bNARr6oK/vc/fw5mfnt2dt34+pycurnvg4eEjgJeRCRZgk553br54XUbN/rAz872VfvNmV++a1e45BJfbf/AA3D33XDssb7n/UUX+b8NfV5Wlg/6gHO+JF9WFn/Yn5k/JjfXP3Jy/D7l5XU3AG08N740nwJeRCTZMjN9B7jomfKKi31I5uU1r8RcWAjnn++r6v/4Rz8r3ne/60vyF17o2+ubCt8gwBuaCS+4ASgv9532qqv9kMCgV39Ghj82tgYguhZANwDtjgJeRCRVzHy7eH6+75BXXOxXtauq8iGZm5v4ZxUUwPTpcPrpMHcu3HknfO97MGKED/qJE1sesvFuAILpfANB6JeVxW/XD24AghqA6PDPytINQBtQwIuIpEPQKa9rV98RL6i+z8ryNwCJBmBeHpxxBpxyil/Y5o47fOl+r7180JeX++F3H3/sF9SZOROOO671599UW311tb9xKS/31fvO1c0VENxARPcDyM6ufxOQjOV7pR4FvIhIOkV3yisv98PbtmzxYZiXl/iCMjk5PuS/+1148km/at255/rPD4J1wwa47DL/PBkh35imbgBqavxNQGmpv7GJnSgoI6N+DUB2dv0aAN0ANJsCXkSkreTm+gVuevTwwffFF74NPOiUl4isLD8j3tSpMHKkbwKItn2775W/ZYufY3/33f3f/Pzk/57GZGTU9eaPJ/oGoLi4rhYANBKghRTwIiJtLbZT3pdf+pDLyPBBnEjpNTPTHxdPSQlceWX9bb161Q/86L99+7b+NzVXUzcAiYwEyM6uqwHIydmxBmAn6weggBcRaS+iO+UFy9Z++aVv2w6CqzF9+/pq+Vj9+sHTT8OHH8JHH9X/+8Yb8NRT9TvOZWZyYM+efhKf6OAPnu+6a/qrzBMdCVBW5vs4VFfXrwEIjo+uAYhuBgjhSAAFvIhIe5SdDd27+05527f7TnnFxT6kGhpTP3Omb3OPXmUuP99v33VX/xg9esfjgolx1q2rDf4t//43eVu3+hn1Pvus/v55ef6mIQj92FqArl2Tey0S0dQNACQ2EiC6BiBoAgj+drAbAAW8iEh7lpGxY6e8zZv9e7Gd8oKOdDfc0Lxe9FlZPpgHDKjd9O7KlfQeNsy/2L7dL2nbUA3Ali31P2+XXRqu/m+L9v9AIiMBghuA6JEAQbBHjwQIZimMrgFoZx0BFfAiIh1FdKe8khJfqo/tlHfcccnvMZ+fD0OH+kc8W7bUBX50+K9e7WsAysrq799U+39zp/dNlkRHApSU7DgSwDl/bDsaCaCAFxHpaDIzoUsXX1KO7pSXmemDPt0lyS5d/GOffXZ8zzm/AE4z2v/Zbbcdg78t2/8DLR0JENQAZGT4lQijpxFOIQW8iEhHFa9T3qZNPmSCKuS2bjc2S6z9P94NQFERfPpp/f2j2//j1QJ07dp2v7mpG4DSUv/PKU0U8CIiYRDdKW/bNh/027bVr0YOxpMHncbaQ5txnPb/erZv9yMDYqv/P/oI3nyzrj9CoLCwfqm/vbT/twEFvIhImARLxgbVwMEUssGjvLzuEduTPAj9RGfTS4f8fPjqV/0jnq1b45f+16zxNQAdpf0/BdrRP0UREUm6oJNXvIVtamp86Ac3ARUVPhArKvx7JSV1bchm7XPI2C67+Lb/htr/v/gi/g3Am2/6uQGqqur2z8jwId8e2/9bQAEvIrKzCuZ/j2fdOj/RTXADUFFRdwMQLCkbzHsfhH97mzfezJfYe/WC/fff8f2qKvjkk/rBHzz/xz/8e9Fyc6F//5a1/z/xBFx/ve9vsPvuMHs2nHpq8n9zFAW8iIjEFz1xTKdO9d+rrq5f/R9U+1dU1C8Vgw/89jhjXFaWD+z+/eO/X1bmx/9Hh3+i7f/Rf1etgptvrmsuWLfOL/0LKQ15BbyIiDRfENbxagCC4WLRpf/otv+amrrSf9DzvD11/Avk5SW3/T/atm1wxRUKeBER6UAaGy4WzBkfXfoP2v2jZ5AL2v1jS//tSaLt/8ccE//4Dz9M6ekp4EVEJH2i54wPOv516VL3fkMd/8rL68+xDzuW/ttL1T/Ub//v1y/+IkC7757SU1DAi4hI+9FYx7+g9B+U/Csr66r9t2/fcQnZ9tLxL94iQAUFvqNdCingRUSkY2hqxbjodv/KyvrD/ior6/f6D6r+g9J/KgVrA6gXvYiISAs01k4f2/Ev6PFfVuYfsTP+RZf+k1H1f9xxMGkS9OwJ3bq1/vMSoIAXEZHwi+34F73gS0Md/5qq+m+PHf+iKOBFRGTn1lTHv+jwD0r/0W3/QbV/sGRsO+n4p4AXERFpTOyY/8LCuvecq9/rv7Kyrt0/drGfioq0nrYCXkREpKXMfLV/Q4vUxHb8y8tL26kp4EVERFKlDdvp29GcgCIiIpIsCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCKQ14M5tsZu+Z2Wozmxnn/YFm9nczW2FmRWbWP+q9aWb2fuQxLZXnKSIiEjYpC3gzywTuBI4C9gZONrO9Y3b7DfCgc244cA1wfeTY7sDVwFhgDHC1mXVL1bmKiIiETSpL8GOA1c65Nc65CmAuMCVmn72BFyLPX4x6fxKwyDm3yTn3JbAImJzCcxUREQmVrBR+dj/go6jX6/El8mhvAccBtwFTgUIz69HAsf1iv8DMpgPTAXr37k1RUVGyzr1DKykp0bVIE13r9NL1Th9d6/RKxfVOZcAn4ifAHWZ2BvASsAGoTvRg59wcYA7A6NGj3fjx41Nwih1PUVERuhbpoWudXrre6aNrnV6puN6pDPgNwICo1/0j22o55z7Gl+Axs87A8c65zWa2ARgfc2xRCs9VREQkVFLZBr8UGGpmg80sBzgJmB+9g5n1NLPgHC4H7o08XwhMNLNukc51EyPbREREJAEpC3jnXBVwPj6Y3wUed86tNLNrzOyYyG7jgffMbBXQG5gdOXYT8Ev8TcJS4JrINhEREUlAStvgnXMLgAUx266Kej4PmNfAsfdSV6IXERGRZtBMdiIiIiGkgBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREFLAi4iIhJACXkREJIQU8CIiIiGkgBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREFLAi4iIhJACXkREJIQU8CIiIiGkgBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREFLAi4iIhJACXkREJIQU8CIiIiGkgBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREFLAi4iIhJACXkREJIQU8CIiIiGkgBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREFLAi4iIhJACXkREJIQU8CIiIiGkgBcREQkhBbyIiEgIKeBFRERCSAEvIiISQgp4ERGREEo44M0s38y+lsqTERERkeRIKODN7NvAcuDZyOuRZjY/lScmIiIiLZdoCX4WMAbYDOCcWw4MTtE5iYiISCslGvCVzrktMdtcsk9GREREkiMrwf1WmtkpQKaZDQUuAJak7rRERESkNRItwc8AhgHlwCPAFuCiVJ2UiIiItE6TJXgzywSecc4dBlyR+lMSERGR1mqyBO+cqwZqzKxLGs5HREREkiDRNvgS4N9mtggoDTY65y5IyVmJiIhIqyQa8E9EHiIiItIBJBTwzrkHzCwH2COy6T3nXGXqTktERERaI6GAN7PxwAPAWsCAAWY2zTn3UupOTURERFoq0Sr6m4CJzrn3AMxsD+BRYP9UnZiIiIi0XKLj4LODcAdwzq0CslNzSiIiItJaiZbgl5nZPcAfI69PBZal5pRERESktRItwZ8HvIOfovaCyPPzmjrIzCab2XtmttrMZsZ5f3cze9HM3jSzFWZ2dGT7IDPbbmbLI4+7Ev9JIiIikmgJPgu4zTl3M9TObpfb2AGRfe4EjgTWA0vNbL5z7p2o3a4EHnfO/c7M9gYWAIMi733gnBuZ8C8RERGRWomW4P8O5Ee9zgeeb+KYMcBq59wa51wFMBeYErOPA3aJPO8CfJzg+YiIiEgjEi3B5znnSoIXzrkSMyto4ph+wEdRr9cDY2P2mQU8Z2YzgE7AEVHvDTazN4GtwJXOucWxX2Bm04HpAL1796aoqCixXxNyJSUluhZpomudXrre6aNrnV6puN6JBnypme3nnHsDwMxGA9uT8P0nA/c7524ys4OAh8xsH+B/wO7OuY1mtj/wpJkNc85tjT7YOTcHmAMwevRoN378+CScUsdXVFSErkV66Fqnl653+uhap1cqrneiAX8R8CczC6rQdwNObOKYDcCAqNf9I9uifR+YDOCce8XM8oCezrnP8EvT4px73cw+wM+ip577IiIiCWi0Dd7MDjCzPs65pcCewGNAJfAs8N8mPnspMNTMBkemuT0JmB+zz4fAhMh37QXkAZ+bWa9IJz3MbAgwFFjTrF8mIiKyE2uqk93dQEXk+UHAz/A9478kUjXeEOdcFXA+sBB4F99bfqWZXWNmx0R2+zFwtpm9hZ8Z7wznnAMOAVaY2XJgHnCuc25Ts3+diIjITqqpKvrMqGA9EZjjnPsz8OdI+DbKObcAP/QtettVUc/fAQ6Oc9yfgT839fkiIiISX1Ml+EwzC24CJgAvRL2XaPu9iIiIpFlTIf0o8A8z+wLfa34xgJl9FdiS4nMTERGRFmo04J1zs83s7/he889F2sfBl/xnpPrkREREpGWarGZ3zr0aZ9uq1JyOiIiIJEOiU9WKiIhIB6KAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIZTSgDezyWb2npmtNrOZcd7f3cxeNLM3zWyFmR0d9d7lkePeM7NJqTxPERGRsMlK1QebWSZwJ3AksB5YambznXPvRO12JfC4c+53ZrY3sAAYFHl+EjAM6As8b2Z7OOeqU3W+IiIiYZLKEvwYYLVzbo1zrgKYC0yJ2ccBu0SedwE+jjyfAsx1zpU75/4LrI58noiIiCQglQHfD/go6vX6yLZos4Dvmdl6fOl9RjOOFRERkQakrIo+QScD9zvnbjKzg4CHzGyfRA82s+nAdIDevXtTVFSUmrPsYEpKSnQt0kTXOr10vdNH1zq9UnG9UxnwG4ABUa/7R7ZF+z4wGcA594qZ5QE9EzwW59wcYA7A6NGj3fjx45N17h1aUVERuhbpoWudXrre6aNrnV6puN6prKJfCgw1s8FmloPvNDc/Zp8PgQkAZrYXkAd8HtnvJDPLNbPBwFDgXyk8VxERkVBJWQneOVdlZucDC4FM4F7n3EozuwZY5pybD/wY+L2ZXYzvcHeGc84BK83sceAdoAr4kXrQi4iIJC6lbfDOuQX4znPR266Kev4OcHADx84GZqfy/ERERMJKM9mJiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAgp4EVEREJIAS8iIhJCCngREZEQUsCLiIiEkAJeREQkhBTwIiIiIaSAFxERCSEFvIiISAilNODNbLKZvWdmq81sZpz3bzGz5ZHHKjPbHPVeddR781N5niIiImGTlaoPNrNM4E7gSGA9sNTM5jvn3gn2cc5dHLX/DGBU1Edsd86NTNX5iYiIhFkqS/BjgNXOuTXOuQpgLjClkf1PBh5N4fmIiIjsNFIZ8P2Aj6Jer49s24GZDQQGAy9Ebc4zs2Vm9qqZHZu60xQREQmflFXRN9NJwDznXHXUtoHOuQ1mNgR4wcz+7Zz7IPogM5sOTAfo3bs3RUVFaTvh9qykpETXIk10rdNL1zt9dK3TKxXXO5UBvwEYEPW6f2RbPCcBP4re4JzbEPm7xsyK8O3zH8TsMweYAzB69Gg3fvz4ZJx3h1dUVISuRXroWqeXrnf66FqnVyqudyqr6JcCQ81ssJnl4EN8h97wZrYn0A14JWpbNzPLjTzvCRwMvBN7rIiIiMSXshK8c67KzM4HFgKZwL3OuZVmdg2wzDkXhP1JwFznnIs6fC/gbjOrwd+E3BDd+15EREQal9I2eOfcAmBBzLarYl7PinPcEmDfVJ6biIhImGkmOxERkRBSwIuIiISQAl5ERCSEFPAiIiIhpIAXEREJIQW8iIhICCngRUREQkgBLyIiEkIKeBERkRBSwIuIiISQAl5ERCSEFPAiIiIhpIAXEREJIQW8iIhICCngRUREQkgBLyIiEkIKeBERkRBSwIuIiISQAl5ERCSEFPAiIiIhpIAXEREJIQW8iIhICCngRUREQkgBLyIiEkIKeBERkRBSwIuIiISQAl5ERCSEFPAiIiIhpIAXEREJIQW8iIhICCngRUREQkgBLyIiEkIKeBERkRBSwIuIiISQAl5ERCSEFPAiIiIhpIAXEREJIQW8iIhICCngRUREQkgBLyIiEkIKeBERkRBSwIuIiIRQVlufgIiISJhV1VRRUV3B9srtFGQXkJ+dn5bvVcCLiIgkUWV1JRXVFZRWllJSUUJVTRXOOapqquhb2FcBLyIi0t4556iorqCiuoKSihJKK0uprqnGzMi0THIyc8jLygOgtKI0reemgBcREUlQjauhorqC8qpySipK2Fa5DeccAFmZWeRl5ZFh7aN7mwJeRESkAdU11b79vGo7JRUllFWV4ZzDzMjJzKEguwAza+vTjEsBLyIiEhG0n2+r3EZJRQkV1RWYGRmWQXZGNp1zOrf1KSZMAS8iIjsl5xyVNZEOcRWllFaUUlVTBUBmRibZmdkUZhW28Vm2nAJeRER2CtHt56WVPtBrXA2GkZWZRU5WDnmW19anmTQKeBERCaWg/bysqoySihK2V20H8IGekdWu28+TQQEvIiKhEEwos62irv3c4Xz7eWY2nbI7hTrQYyngRUSkw4nXfl5ZU4lhte3nnbM6Toe4VFDAi4hIuxdMKFPbfl7p28+BugllssPTfp4MCngREWl3alwN5VXllFWVUVpRyrYqP6GMYWRnZrerCWXaKwW8iIi0uegFWUoqSiivKgfAzHbK9vNkUMCLhISdFQAACfZJREFUiEjaxS7IUlldCfjx5zmZOXTO3bnbz5NBAS8iIinVnAVZJHkU8CIiklQdaUGWMFPAi4hIq1TXVFNeXV43oUxlZEKZDrAgS5gp4EVEpFmaWpClMLfjzt8eJikNeDObDNwGZAL3OOduiHn/FuCwyMsCYFfnXNfIe9OAKyPvXeuceyCV5yoiIjuKnlCmpNxXt1fVVOFwZGVkdfgFWcIsZQFvZpnAncCRwHpgqZnNd869E+zjnLs4av8ZwKjI8+7A1cBowAGvR479MlXnKyIide3n1a6aDVs3sK1yW6gXZAmzVJbgxwCrnXNrAMxsLjAFeKeB/U/GhzrAJGCRc25T5NhFwGTg0RSer4jITqehBVmCcelqP++4Uhnw/YCPol6vB8bG29HMBgKDgRcaObZfCs5RRGSnUlVTRXlVed2EMtV+QpnYBVkyLIPcrNw2PltpjfbSye4kYJ5zrro5B5nZdGB65GWJmb2X9DPrmHoCX7T1SewkdK3TS9e7+SzyvwwyyACC4rjD4Ro8qobuZLApLWe4szAyqKGKGuJlXUv/3R7Y0BupDPgNwICo1/0j2+I5CfhRzLHjY44tij3IOTcHmNOakwwjM1vmnBvd1uexM9C1Ti9d7/Qxs2WuStc6XVLx73YqZxpYCgw1s8FmloMP8fmxO5nZnkA34JWozQuBiWbWzcy6ARMj20RERCQBKSvBO+eqzOx8fDBnAvc651aa2TXAMudcEPYnAXNdMM2RP3aTmf0Sf5MAcE3Q4U5ERESaZlG5KiFhZtMjzReSYrrW6aXrnT661umViuutgBcREQkhzfYvIiISQgr4DsLM7jWzz8zs7aht3c1skZm9H/nbLbLdzOx2M1ttZivMbL+oY6ZF9n8/Mh2wxDCzAWb2opm9Y2YrzezCyHZd7yQzszwz+5eZvRW51r+IbB9sZq9FruljkY66mFlu5PXqyPuDoj7r8sj298xsUtv8ovbPzDLN7E0zezryWtc6RcxsrZn928yWm9myyLb0/XfEOadHB3gAhwD7AW9HbbsRmBl5PhP4VeT50cDf8ONdDwRei2zvDqyJ/O0Wed6trX9be3sAuwH7RZ4XAquAvXW9U3KtDegceZ4NvBa5ho8DJ0W23wWcF3n+Q+CuyPOTgMciz/cG3gJy8ZNmfQBktvXva48P4BLgEeDpyGtd69Rd67VAz5htafvviErwHYRz7iXYYdKJKUCwCM8DwLFR2x903qtAVzPbjagpgJ2f1z+YAliiOOf+55x7I/K8GHgXP5OirneSRa5ZSeRlduThgMOBeZHtsdc6+GcwD5hgfh7VKfjROOXOuf8Cq/HTZUsUM+sPfBO4J/La0LVOt7T9d0QB37H1ds79L/L8E6B35HlDU/1qCuBmilRLjsKXLHW9UyBSZbwc+Az/H68PgM3OuarILtHXrfaaRt7fAvRA1zpRtwKXATWR1z3QtU4lBzxnZq+bn3kV0vjfkfYyVa20knPOmZmGRCSRmXUG/gxc5JzbalELbuh6J4/zU1SPNLOuwF+APdv4lELJzL4FfOace93Mxrf1+ewkvuGc22BmuwKLzOw/0W+m+r8jKsF3bJ9GqnCI/P0ssr2haYKbM33wTs3MsvHh/rBz7onIZl3vFHLObQZeBA7CV08GBZDo61Z7TSPvdwE2omudiIOBY8xsLTAXXzV/G7rWKeOc2xD5+xn+5vX/27u/EKnKMI7j319ZmpWCUhchYRvb/0TBRNSswIq8kwSDwLak7J9SIUQIIQRlFi5KkkJJXYgXVpQllZQu0Vr5J01X0xTrwv4QdCFu5WL6dPG+wx7GXd1qd2fn9PvAwJx33nPmPWeYefac8+7zTKAff0cc4OvbeqAyo/I+4L1C++w8K3MicDRfEnIK4B7I9xlfB76NiKWFl3y8e5mkS/KZO5IuAG4nzXnYDMzM3aqPdeUzmAlsijQTaT1wT575fQXQCGztn72oDxHxTESMiojRpElzmyLiXnys+4SkCyVdXHlO+v630Z+/I7WeZehHj2djrgV+Bk6Q7sHMId0P+xQ4CHwCjMh9Bawg3cvcA4wvbOcB0qSYQ8D9td6vgfgAppDune0GduXHdB/vPjnWY4Cd+Vi3Ac/m9gZS0DgErAMG5/YheflQfr2hsK2F+TM4ANxV630byA9SMa/KLHof6745xg2k/zb4BtgLLMzt/fY74kx2ZmZmJeRL9GZmZiXkAG9mZlZCDvBmZmYl5ABvZmZWQg7wZmZmJeQAbzaASBqZK0/tkvSLpB8Ly+efZd3xkpb34D229N6Ia09Sk6RXaj0Os4HGqWrNBpCI+A0YCyBpEdAeES9XXpc0KDrzhlevux3Y3oP3mNQ7ozWzgcxn8GYDnKQ3JK2U9BWwRNIESV/kmt5bJF2d+92qzhrfiyStltQi6bCk+YXttRf6t0h6S9J+SWtyFj8kTc9tO3KN6g+6GNe5kl6StC3Xr56b25+UtDo/v1FSm6ShZxh3k6R3lWpj/yDpcUlP5X5fShqR+7VIWpavZrRJOq2CWc6M93Ye0zZJk3P7LYUrITsrGcbMysxn8Gb1YRQwKSJOShoG3BwRf0maBjwP3N3FOtcAt5Fq2h+Q9GpEnKjqMw64HvgJaAUmS9oOrAKmRsT3ktZ2M6Y5pHSaN0kaDLRK2kjKb94iaQYp49nciPhDqdBGd+O+IY9lCClb19MRMU5SMzCbVAUNYGhEjJU0FVid1ytaBjRHxOeSLiel9LwWWAA8FhGtSkWEjnezT2al4QBvVh/WRaq6Bqnox5uSGkkpdc/rZp0NEdEBdEj6lVSW8khVn60RcQRAqWTraKAdOByp1jekNMkPcbo7gDGSKnnMhwON+Y+CJlL62VUR0dqDcW+OiGPAMUlHgfdz+x5SOtuKtQAR8ZmkYZU89gXTgOvUWflvWA7orcBSSWuAdyr7bFZmDvBm9eH3wvPnSAFxhlK9+pZu1ukoPD9J19/3nvTpjoB5EdFV4YtG0h8KlxXazjTu4jhOFZZPVY2pOrd29fI5wMSIqD5DXyxpA6mmQKukOyNiP2Yl5nvwZvVnOJ3lIpv6YPsHgIYchAFmddPvY+ARpdK6SLoqV9AaDiwHpgIjq87w/+u4Z+X3mkK6PXC06vWNwLzKgqTKhMUrI2JPRLwIbMM15+1/wAHerP4sAV6QtJM+uAoXEX8CjwIfSdoBHAOqAynAa8A+4GtJbaT79oOAZmBFRHxHuk+/WNKlvTTu43n9lXnb1eYD4/Okv33Aw7n9iTwxbzepIuOH//L9zeqGq8mZ2WkkXRQR7XlW/QrgYEQ013hMLcCC/O+AZnYWPoM3s648mCfd7SVdWl9V4/GY2T/kM3gzM7MS8hm8mZlZCTnAm5mZlZADvJmZWQk5wJuZmZWQA7yZmVkJOcCbmZmV0N9CkLQxwVH/AQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x576 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"4TsaXkfFgClu"},"source":["### PR Curve"],"id":"4TsaXkfFgClu"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":762},"id":"a68845b8","executionInfo":{"status":"error","timestamp":1634110620573,"user_tz":-420,"elapsed":851,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"2ce6c9c1-a406-4066-94a4-ff51f08f8ee4"},"source":["pr_viz = PrecisionRecallCurve(elmo_svm_clf,\n","                              per_class=True,\n","                              cmap=\"Set1\",\n","                              micro=True)\n","pr_viz.fit(X_train, y_train)\n","pr_viz.score(X_test, y_test)\n","viz.show()"],"id":"a68845b8","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n","  FutureWarning)\n"]},{"output_type":"error","ename":"YellowbrickValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mYellowbrickValueError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-96-14ef9e1f5079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Set1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               micro=True)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpr_viz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpr_viz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/yellowbrick/classifier/prcurve.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;34m\"please provide a binary or multiclass single-output target\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             ))\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mYellowbrickValueError\u001b[0m: PrecisionRecallCurve does not support target type 'multilabel-indicator', please provide a binary or multiclass single-output target"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"947ec3a7"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n","from sklearn.model_selection import train_test_split as tts\n","from yellowbrick.classifier import PrecisionRecallCurve\n","from yellowbrick.datasets import load_game\n","\n","# Load dataset and encode categorical variables\n","X, y = load_game()\n","X = OrdinalEncoder().fit_transform(X)\n","y = LabelEncoder().fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, shuffle=True)\n","\n","# Create the visualizer, fit, score, and show it\n","viz = PrecisionRecallCurve(\n","    RandomForestClassifier(n_estimators=10),\n","    per_class=True,\n","    micro=False,\n","    cmap=\"Set1\"\n",")\n","viz.fit(X_train, y_train)\n","viz.score(X_test, y_test)\n","viz.show()"],"id":"947ec3a7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W1rySE4dmdF3"},"source":["### Classification Report"],"id":"W1rySE4dmdF3"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41W_7dj_mKOJ","executionInfo":{"status":"ok","timestamp":1634116093160,"user_tz":-420,"elapsed":771,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"9b90d5e5-dedb-4c58-ab80-6d88a341f8d5"},"source":["y_pred = clf.predict(X_test)\n","print(classification_report(y_test, y_pred, target_names=mlb.classes_))"],"id":"41W_7dj_mKOJ","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                  precision    recall  f1-score   support\n","\n","               Agent_CallPurpose       0.69      0.46      0.55        24\n","            Agent_ChildEducation       0.00      0.00      0.00         0\n","       Agent_CompanyIntroduction       0.00      0.00      0.00         0\n","                       Agent_EMI       0.50      0.35      0.41        17\n","       Agent_ExplainDocsRequired       0.60      0.14      0.23        21\n","Agent_ExplainMoneyTranferProcess       0.00      0.00      0.00         0\n","            Agent_FamilyShopping       1.00      0.20      0.33         5\n","                 Agent_GoodClose       0.00      0.00      0.00         5\n","                 Agent_Greetings       0.38      0.38      0.38        13\n","               Agent_HouseFixing       0.00      0.00      0.00         0\n","            Agent_InformCallBack       0.00      0.00      0.00         5\n","              Agent_InsuranceFee       0.00      0.00      0.00         2\n","              Agent_InterestRate       0.57      0.22      0.32        18\n","            Agent_ListeningSkill       0.45      0.38      0.42        13\n","                Agent_LoanAmount       0.50      0.12      0.20         8\n","              Agent_LoanDuration       0.00      0.00      0.00         0\n","      Agent_MentioningDisclaimer       1.00      0.29      0.44         7\n","             Agent_OH_AmountLess       0.00      0.00      0.00         6\n","          Agent_OH_BadExperience       0.00      0.00      0.00         0\n","  Agent_OH_CustAskForCallingBack       0.00      0.00      0.00         2\n","      Agent_OH_DiscussWithFamily       0.00      0.00      0.00         1\n","           Agent_OH_HighInterest       0.00      0.00      0.00         1\n","       Agent_OH_HighLoanDuration       0.00      0.00      0.00         2\n","    Agent_OH_NotRequireInsurance       0.00      0.00      0.00         0\n","         Agent_OH_NotRequireLoan       0.00      0.00      0.00        26\n","               Agent_RunBusiness       1.00      0.25      0.40         4\n","         Agent_Self Introduction       0.89      0.85      0.87        20\n","                 Agent_Summarize       0.00      0.00      0.00         3\n","             Agent_ThanksClosing       0.40      0.40      0.40        10\n","        Agent_VerifyCustomerName       0.67      0.25      0.36         8\n","               Client_AmountLess       0.00      0.00      0.00         3\n","        Client_AskForCallingBack       0.00      0.00      0.00         4\n","            Client_BadExperience       0.00      0.00      0.00         2\n","        Client_DiscussWithFamily       0.00      0.00      0.00         3\n","             Client_HighInterest       0.00      0.00      0.00         2\n","         Client_HighLoanDuration       0.00      0.00      0.00         2\n","           Client_NotRequireLoan       0.35      0.38      0.36        24\n","                           other       0.88      0.92      0.90      1000\n","\n","                       micro avg       0.84      0.78      0.81      1261\n","                       macro avg       0.26      0.15      0.17      1261\n","                    weighted avg       0.79      0.78      0.78      1261\n","                     samples avg       0.79      0.79      0.79      1261\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqqTjYQNFM3M","executionInfo":{"status":"ok","timestamp":1634135784641,"user_tz":-420,"elapsed":967,"user":{"displayName":"Huy Bùi Gia","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09854117822191732358"}},"outputId":"e48052cb-b66a-4de9-a7f6-6624533bc3cf"},"source":["!git pull another ma"],"id":"PqqTjYQNFM3M","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["warning: no common commits\n","remote: Enumerating objects: 222, done.\u001b[K\n","remote: Counting objects:   2% (1/48)\u001b[K\rremote: Counting objects:   4% (2/48)\u001b[K\rremote: Counting objects:   6% (3/48)\u001b[K\rremote: Counting objects:   8% (4/48)\u001b[K\rremote: Counting objects:  10% (5/48)\u001b[K\rremote: Counting objects:  12% (6/48)\u001b[K\rremote: Counting objects:  14% (7/48)\u001b[K\rremote: Counting objects:  16% (8/48)\u001b[K\rremote: Counting objects:  18% (9/48)\u001b[K\rremote: Counting objects:  20% (10/48)\u001b[K\rremote: Counting objects:  22% (11/48)\u001b[K\rremote: Counting objects:  25% (12/48)\u001b[K\rremote: Counting objects:  27% (13/48)\u001b[K\rremote: Counting objects:  29% (14/48)\u001b[K\rremote: Counting objects:  31% (15/48)\u001b[K\rremote: Counting objects:  33% (16/48)\u001b[K\rremote: Counting objects:  35% (17/48)\u001b[K\rremote: Counting objects:  37% (18/48)\u001b[K\rremote: Counting objects:  39% (19/48)\u001b[K\rremote: Counting objects:  41% (20/48)\u001b[K\rremote: Counting objects:  43% (21/48)\u001b[K\rremote: Counting objects:  45% (22/48)\u001b[K\rremote: Counting objects:  47% (23/48)\u001b[K\rremote: Counting objects:  50% (24/48)\u001b[K\rremote: Counting objects:  52% (25/48)\u001b[K\rremote: Counting objects:  54% (26/48)\u001b[K\rremote: Counting objects:  56% (27/48)\u001b[K\rremote: Counting objects:  58% (28/48)\u001b[K\rremote: Counting objects:  60% (29/48)\u001b[K\rremote: Counting objects:  62% (30/48)\u001b[K\rremote: Counting objects:  64% (31/48)\u001b[K\rremote: Counting objects:  66% (32/48)\u001b[K\rremote: Counting objects:  68% (33/48)\u001b[K\rremote: Counting objects:  70% (34/48)\u001b[K\rremote: Counting objects:  72% (35/48)\u001b[K\rremote: Counting objects:  75% (36/48)\u001b[K\rremote: Counting objects:  77% (37/48)\u001b[K\rremote: Counting objects:  79% (38/48)\u001b[K\rremote: Counting objects:  81% (39/48)\u001b[K\rremote: Counting objects:  83% (40/48)\u001b[K\rremote: Counting objects:  85% (41/48)\u001b[K\rremote: Counting objects:  87% (42/48)\u001b[K\rremote: Counting objects:  89% (43/48)\u001b[K\rremote: Counting objects:  91% (44/48)\u001b[K\rremote: Counting objects:  93% (45/48)\u001b[K\rremote: Counting objects:  95% (46/48)\u001b[K\rremote: Counting objects:  97% (47/48)\u001b[K\rremote: Counting objects: 100% (48/48)\u001b[K\rremote: Counting objects: 100% (48/48), done.\u001b[K\n","remote: Compressing objects:   2% (1/39)\u001b[K\rremote: Compressing objects:   5% (2/39)\u001b[K\rremote: Compressing objects:   7% (3/39)\u001b[K\rremote: Compressing objects:  10% (4/39)\u001b[K\rremote: Compressing objects:  12% (5/39)\u001b[K\rremote: Compressing objects:  15% (6/39)\u001b[K\rremote: Compressing objects:  17% (7/39)\u001b[K\rremote: Compressing objects:  20% (8/39)\u001b[K\rremote: Compressing objects:  23% (9/39)\u001b[K\rremote: Compressing objects:  25% (10/39)\u001b[K\rremote: Compressing objects:  28% (11/39)\u001b[K\rremote: Compressing objects:  30% (12/39)\u001b[K\rremote: Compressing objects:  33% (13/39)\u001b[K\rremote: Compressing objects:  35% (14/39)\u001b[K\rremote: Compressing objects:  38% (15/39)\u001b[K\rremote: Compressing objects:  41% (16/39)\u001b[K\rremote: Compressing objects:  43% (17/39)\u001b[K\rremote: Compressing objects:  46% (18/39)\u001b[K\rremote: Compressing objects:  48% (19/39)\u001b[K\rremote: Compressing objects:  51% (20/39)\u001b[K\rremote: Compressing objects:  53% (21/39)\u001b[K\rremote: Compressing objects:  56% (22/39)\u001b[K\rremote: Compressing objects:  58% (23/39)\u001b[K\rremote: Compressing objects:  61% (24/39)\u001b[K\rremote: Compressing objects:  64% (25/39)\u001b[K\rremote: Compressing objects:  66% (26/39)\u001b[K\rremote: Compressing objects:  69% (27/39)\u001b[K\rremote: Compressing objects:  71% (28/39)\u001b[K\rremote: Compressing objects:  74% (29/39)\u001b[K\rremote: Compressing objects:  76% (30/39)\u001b[K\rremote: Compressing objects:  79% (31/39)\u001b[K\rremote: Compressing objects:  82% (32/39)\u001b[K\rremote: Compressing objects:  84% (33/39)\u001b[K\rremote: Compressing objects:  87% (34/39)\u001b[K\rremote: Compressing objects:  89% (35/39)\u001b[K\rremote: Compressing objects:  92% (36/39)\u001b[K\rremote: Compressing objects:  94% (37/39)\u001b[K\rremote: Compressing objects:  97% (38/39)\u001b[K\rremote: Compressing objects: 100% (39/39)\u001b[K\rremote: Compressing objects: 100% (39/39), done.\u001b[K\n","Receiving objects:   0% (1/222)   \rReceiving objects:   1% (3/222)   \rReceiving objects:   2% (5/222)   \rReceiving objects:   3% (7/222)   \rReceiving objects:   4% (9/222)   \rReceiving objects:   5% (12/222)   \rReceiving objects:   6% (14/222)   \rReceiving objects:   7% (16/222)   \rReceiving objects:   8% (18/222)   \rReceiving objects:   9% (20/222)   \rReceiving objects:  10% (23/222)   \rReceiving objects:  11% (25/222)   \rReceiving objects:  12% (27/222)   \rReceiving objects:  13% (29/222)   \rReceiving objects:  14% (32/222)   \rReceiving objects:  15% (34/222)   \rReceiving objects:  16% (36/222)   \rReceiving objects:  17% (38/222)   \rReceiving objects:  18% (40/222)   \rReceiving objects:  19% (43/222)   \rReceiving objects:  20% (45/222)   \rReceiving objects:  21% (47/222)   \rReceiving objects:  22% (49/222)   \rReceiving objects:  23% (52/222)   \rReceiving objects:  24% (54/222)   \rReceiving objects:  25% (56/222)   \rReceiving objects:  26% (58/222)   \rReceiving objects:  27% (60/222)   \rReceiving objects:  28% (63/222)   \rReceiving objects:  29% (65/222)   \rReceiving objects:  30% (67/222)   \rReceiving objects:  31% (69/222)   \rReceiving objects:  32% (72/222)   \rReceiving objects:  33% (74/222)   \rReceiving objects:  34% (76/222)   \rReceiving objects:  35% (78/222)   \rReceiving objects:  36% (80/222)   \rReceiving objects:  37% (83/222)   \rremote: Total 222 (delta 17), reused 18 (delta 8), pack-reused 174\u001b[K\n","Receiving objects:  38% (85/222)   \rReceiving objects:  39% (87/222)   \rReceiving objects:  40% (89/222)   \rReceiving objects:  41% (92/222)   \rReceiving objects:  42% (94/222)   \rReceiving objects:  43% (96/222)   \rReceiving objects:  44% (98/222)   \rReceiving objects:  45% (100/222)   \rReceiving objects:  46% (103/222)   \rReceiving objects:  47% (105/222)   \rReceiving objects:  48% (107/222)   \rReceiving objects:  49% (109/222)   \rReceiving objects:  50% (111/222)   \rReceiving objects:  51% (114/222)   \rReceiving objects:  52% (116/222)   \rReceiving objects:  53% (118/222)   \rReceiving objects:  54% (120/222)   \rReceiving objects:  55% (123/222)   \rReceiving objects:  56% (125/222)   \rReceiving objects:  57% (127/222)   \rReceiving objects:  58% (129/222)   \rReceiving objects:  59% (131/222)   \rReceiving objects:  60% (134/222)   \rReceiving objects:  61% (136/222)   \rReceiving objects:  62% (138/222)   \rReceiving objects:  63% (140/222)   \rReceiving objects:  64% (143/222)   \rReceiving objects:  65% (145/222)   \rReceiving objects:  66% (147/222)   \rReceiving objects:  67% (149/222)   \rReceiving objects:  68% (151/222)   \rReceiving objects:  69% (154/222)   \rReceiving objects:  70% (156/222)   \rReceiving objects:  71% (158/222)   \rReceiving objects:  72% (160/222)   \rReceiving objects:  73% (163/222)   \rReceiving objects:  74% (165/222)   \rReceiving objects:  75% (167/222)   \rReceiving objects:  76% (169/222)   \rReceiving objects:  77% (171/222)   \rReceiving objects:  78% (174/222)   \rReceiving objects:  79% (176/222)   \rReceiving objects:  80% (178/222)   \rReceiving objects:  81% (180/222)   \rReceiving objects:  82% (183/222)   \rReceiving objects:  83% (185/222)   \rReceiving objects:  84% (187/222)   \rReceiving objects:  85% (189/222)   \rReceiving objects:  86% (191/222)   \rReceiving objects:  87% (194/222)   \rReceiving objects:  88% (196/222)   \rReceiving objects:  89% (198/222)   \rReceiving objects:  90% (200/222)   \rReceiving objects:  91% (203/222)   \rReceiving objects:  92% (205/222)   \rReceiving objects:  93% (207/222)   \rReceiving objects:  94% (209/222)   \rReceiving objects:  95% (211/222)   \rReceiving objects:  96% (214/222)   \rReceiving objects:  97% (216/222)   \rReceiving objects:  98% (218/222)   \rReceiving objects:  99% (220/222)   \rReceiving objects: 100% (222/222)   \rReceiving objects: 100% (222/222), 126.95 KiB | 3.97 MiB/s, done.\n","Resolving deltas:   0% (0/104)   \rResolving deltas:   1% (2/104)   \rResolving deltas:   7% (8/104)   \rResolving deltas:   8% (9/104)   \rResolving deltas:   9% (10/104)   \rResolving deltas:  11% (12/104)   \rResolving deltas:  12% (13/104)   \rResolving deltas:  14% (15/104)   \rResolving deltas:  17% (18/104)   \rResolving deltas:  25% (27/104)   \rResolving deltas:  33% (35/104)   \rResolving deltas:  34% (36/104)   \rResolving deltas:  38% (40/104)   \rResolving deltas:  39% (41/104)   \rResolving deltas:  41% (43/104)   \rResolving deltas:  44% (46/104)   \rResolving deltas:  45% (47/104)   \rResolving deltas:  50% (52/104)   \rResolving deltas:  52% (55/104)   \rResolving deltas:  63% (66/104)   \rResolving deltas:  65% (68/104)   \rResolving deltas:  74% (77/104)   \rResolving deltas:  82% (86/104)   \rResolving deltas:  85% (89/104)   \rResolving deltas:  86% (90/104)   \rResolving deltas:  88% (92/104)   \rResolving deltas:  89% (93/104)   \rResolving deltas:  91% (95/104)   \rResolving deltas:  94% (98/104)   \rResolving deltas:  95% (99/104)   \rResolving deltas:  97% (101/104)   \rResolving deltas:  98% (102/104)   \rResolving deltas: 100% (104/104)   \rResolving deltas: 100% (104/104), done.\n","From https://github.com/hosjiu1702/ELMoForManyLangs\n"," * branch            master     -> FETCH_HEAD\n"," * [new branch]      master     -> another/master\n","fatal: refusing to merge unrelated histories\n"]}]}]}